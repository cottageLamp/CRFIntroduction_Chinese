# 2.2 生成与判别模型

本节我们探讨几个已被用于自然语言处理的简单图模型。虽然它们已被熟知，但它们一方面可以澄清前文提到的诸多概念，另一方面也可以说明某些今后讨论CRFs时会遇到的议题。我们尤其关注隐马尔科夫模型(HMM)，因为它与线性链条件随机场密切相关。

本节的主要目的是对比生成与判别模型。将会提到的模型，包括两个生成模型(朴树贝叶斯和HMM)，一个判别模型(逻辑回归模型)。**生成模型**描述了，一个输出向量y以怎样的概率"生成”输入特征x。**判别模型**从相反的方向工作，直接描述了如何利用输入特征x来给输出y赋值。一般来说，这两者可根据贝叶斯法则互相转化。但在实践中却相去甚远，各自隐藏着一些优点(将在2.2.3节讲述)。


## 2.2.1 分类

我们首先讨论**分类**问题——根据给定的一个向量$$x=(x_1,x_2,...,x_K)$$，来预测单一的y变量的离散值(类别标签)。一个简单的方法是，假定当类别标签已知时，所有的特征是独立的。结果是所谓的朴素贝叶斯分类器。它基于如下的联合概率模型：

$$
p(y,x)=p(y)\prod^K_{k=1}p(x_k|y) (2.7)
$$

这个模型可以描述为图2.3(左)的有向模型。为每个特征$$x_k$$定义因子$$\Psi(y)=p(y)$$，以及因子$$\Psi_k(y,x_k)=p(x_k|y)$$,我们也可以写成因子图。这样的因子图如图2.3(右)所示。  

![](/assets/2.3.png)

图2.3 朴素贝叶斯分类器，被当成有向模型(左)，或因子图(右)

逻辑回归(有时在NLP圈子里叫做**最大熵分类器**)是另一个知名的，且很自然地表达为图模型的分类器。该分类器源于将每个类的逻辑概率，log p(y|x)，假设为x的线性函数，以及一个归一化常数。这导致了如下的条件概率：


$$
p(y|x)=\frac{1}{Z(x)}exp\{\theta_y+\sum^K_{j=1}\theta_{y,j}x_j\} (2.8)
$$

其中$$Z(x)=\sum_y exp\{\theta_y+\sum^K_{j=1}\theta_{y,j}x_j\}$$，是归一化常数。而$$\theta_y$$是偏置量，相当于朴素贝叶斯里面的log p(y)。与其像式(2.8)那样为每一个类制定一个权重向量，我们不如采用被所有类共享的一组权重的记号。这一技巧通过定义一组**特征函数(feature functions)**来实现，而这些特征只对某一类时非零。为了达到这个目的，特征权重的特征函数被定义为$$f_{y',j}(y,x)={1}_{\{y'=y\}}x_j$$，而把偏置权重的特征函数定义为$$f_{y'}(y,x)=1_{\{y'=y\}}$$。现在我们可以用$$f_k$$来遍历每个特征函数$$f_{y',j}$$，用$$\theta_k$$来索引对应的权重$$\theta_{y',j}$$。利用这一符号技巧，逻辑回归模型变成了：

$$
p(y|x)=\frac{1}{Z(x)}exp\{\sum^K_{k=1}\theta_k f_k(y,x)\} (2.9)
$$


我们之所以引入这样的记号，是因为它简化了下文介绍CRFs时的记号。<font color=red>译注：(2.8)中的$$\theta_y$$好像丢失了？</font>

### 2.2.2 序列模型

分类器只对单一变量做预测，但图模型的真正用处在于对大量互相关变量的建模能力。本节，我们讨论了可能是最简单的相关性——图模型中的输出变量被排列成一个序列。为了展示该模型的好处，我们讨论一个自然语言处理中的应用——**命名实体识别(named-entity recognition,NER)**。NER是在文本中识别并分类命名实体，包括地点(如China)，人(如George Bush)和组织(如United Nations)。给定一个句子，命名实体识别任务是把其中的单词切分成几段，每一段对应一个实体，然后对该实体进行分类(类别包括人，组织，地点等等)。该问题的挑战性在于，很多实体的字符串很少见，哪怕在一个很大的训练集上。于是，我们只能根据上下文来识别它们。

一种办法是独立地对每个单词进行分类，看它是一个人、地点、组织或者其他(既不是一个实体)。这种办法的缺点在于：给定输入之后，它假定所有的命名实体标签是独立的。实际上，临近单词的标签是相关的。例如，New York是一个地点，Now York Times却是一个组织。一种缓解这种无关性假设的方法，是把输出变量安排到一个线性链中。这是隐马尔科夫模型(HMM)【111】的方法。一个HMM通过假定一个潜在的**状态**序列$$Y=\{y_t\}^T_{t=1}$$ ，来对一序列的观测$$X=\{x_t\}^T_{t=1}$$ 建模。记S为可能状态的有限集，O为可能观测的有限集，即是说，对于任何的t，$$x_t\in O, y_t\in S$$<font color=red>译注：S包含了所有可能的输出值，O包含了所有可能的输入值</font>。在命名实体例子中，t位置的单词就是观测$$x_t$$ ，而$$y_t$$ 是该位置的标签。

为了可行地对联合分布p(y, x)建模，一个HMM做了两个无关性假设。第一，它假设每个状态只依赖于它的前一个状态，即给定$$y_{t-1}$$ 之后， $$y_t$$ 于$$y_1,y_2,...,y_{t-1}$$ 都无关了。第二，它假定每个观测变量$$x_t$$ 只与对应的状态$$y_t$$ 有关。基于这些假设，我们可用三个概率分布来指明一个HMM。第一个，初始状态的概率布p(y1)；第二个，转移概率$$p(y_t|y_{t-1})$$;最后，观测概率$$p(x_t|y_t)$$。总之，状态序列y于观测序列x的联合分布被分解为：

$$
p(y, x)=\prod^T_{t=1}p*y_t|y_{t-1})p(x_t|y_t) (2.10)
$$

为了简化上式的符号，我们创造了"虚拟”初始状态$$y_0$$，它总是0，并是所有状态序列的起点。这让我们把创始状态概率$$p(y_1)$$写成$$p(y_1|y_0)$$。

HMMs已在自然语言处理中用于很多序列标注任务，如part-of-speech tagging, 命名实体识别和信息提取。

## 2.2.3 比较

生成模型和判别模型都描述了(y,x)的分布，却是从不同的方向。生成模型，如朴素贝叶斯分类器和HMM，是一簇按照p(y,x)=p(y)p(x|y)进行分解的联合分布。也就是说，它描述了如何根据标签采样或”生成"特征。判别模型，如逻辑回归模型，是一簇条件分布$$p(y|x)$$。也就是说，直接对分类规则建模。原理上，利用输入的边缘分布p(x) ，一个判别模型可以被转化成联合分布p(y,x)，然而很少需要这么做。

判别和生成模型在概念上的主要区别，就是条件分布p(y|x)没有包含p(x)的模型，而它对分类并没有用。对p(x)建模的困难性在于，它包含了很多高度相关的特征，而这是很难建模的。如在命名实体识别中，朴素的HMM只依赖于单一的特征——单词本身。然而许多单词，特别是专有名词，却从未在训练集中出现过，因而以单词本身作为特征是缺乏足够的信息的。为了对全新单词进行标注，我们想要利用其它的特征，如它的大小写、它的临近单词、它的前后缀、它在预先确定的一组人或地方中的身份(its membership in predetermined lists of people and locations???)，等等。

判别模型的主要优势在于它适合包含丰富的、重叠的特征。为了理解这一点，考虑一簇朴素贝叶斯分布(2.7)。这簇联合分布的条件部分均采用了"逻辑回归的形式"(2.9)。然而还有很多其他的联合模型，有些带有x 之间的复杂的依赖，而条件分布也采用了(2.9)的形式。为了直接对条件分布建模，我们仍然可以认为p(x)是不可知的。判别模型，如CRF，仅对y的条件独立性做假设，以及y如何依赖于x，但是不对x之间的条件独立性做假设。 这一点也可以通过图形的方式来理解。假定我们有关于联合分布p(y,x)的因子图，现在要构建条件分布p(y|x)的因子图，那么，所有只与x有关的因子都可以消失了。它们与条件部分无关，因为它们关于y是常数。

为了在生成模型中包含互相关的特征，我们有两个选择。一是增强模型以表达输入间的相关性，如在每个$$x_t$$之间增加连接。然而很难可操作地这样做。例如，很难想象如何对单词的大小写以及前后缀之间的相关性建模。亦或者，我们也不想去做这个件事，因为我们总是看得到输入的句子。

第二个办法是只做一些简单的相关性假设，如朴素贝叶斯假设。例如，带有朴素贝叶斯假设的HMM采用了$$p(x, y)=\prod^T_{t=1}p(y_t|y_{t-1})\prod^K_{k=1}p(x_{tk}|y_t)$$的形式。这一思路有时很凑效，但也可能很有问题，因为这一独立性假设会影响性能。例如，虽然朴素贝叶斯分类器在文档分类方面表现优秀，它在许多应用中的平均表现要比逻辑回归差【19】。

而且，朴素贝叶斯可以产生差的概率估计。作为说明的例子，想象朴素贝叶斯在一个二分类问题上训练。现在，我们把输入特征向量$$x=(x_1,x_2,...,x_K)$$重复一下，变换成$$x'=(x_1,x_1,x_2,x_2,...,x_K,x_k)$$，然后运行朴素贝叶斯分类器。虽然没有任何新的信息被加入到数据中，这一变换却增加了概率估计的信心。就是说，朴素贝叶斯对p(y|x')的估计，相比于p(y|x)，更倾向远离0.5。

当我们扩展到序列模型的时候，想朴素贝叶斯那样的假设尤其有问题，因为推断过程需要综合模型不同部分的证据。如果序列的每个位置的标签，其概率估计都偏大，那么很难合理地把它们综合起来。

朴素贝叶斯和逻辑回归之间的差别，正是前者是生成的，而后者是判别的。在输入为离散时，这两个分类器在其他方面完全一致。朴素贝叶斯和逻辑回归考虑了相同的假设空间，因为在相同的决策范围里，任何逻辑回归分类器都可以转变成朴素贝叶斯分类器，反之亦然。再者，朴素贝叶斯模型(2.7)与逻辑回归模型(2.9)定义了相同的分布簇。我们可以生成式地表示(2.7)如下：

$$
p(y,x)=\frac{exp\{\sum_k \theta_kf_k(y,x\}}{\sum_{\hat{y},\hat{x}}\theta_kf_k(\hat{y},\hat{x})} (2.11)
$$

这意味着，如果朴素贝叶斯(2.7)按照极大条件似然来训练，我们会获得与逻辑回归一样的分类器。相反，如果按照生成方法来表示逻辑回归，如(2.11)，并按照最大化联合似然p(y,x)来训练，我们会得到与朴素贝叶斯同样的分类器。按照Ng和Jordan【98】的说法，朴素贝叶斯和逻辑回归构成了**生成-判别对(generative-discriminative pair)**。关于最新的生成与判别模型的理论视角，请参考Liang和Jordan【72】。

原理上，我们可能不清楚这两种方案如此不同的原因，毕竟它们之间可通过贝叶斯法则互相转化。如在朴素贝叶斯模型中，是很容易把联合分布p(y)p(x|y)转化成条件分布p(y|x)的。 实际上，该条件分布与逻辑回归模型(2.9)的形式是一样的。另外如果我们想获得关于数据的“真实”生成模型，即真正把数据产生出来的分布$$p^*(y,x)=p^*(y)p^*(x|y)$$，那么我们只需简单地计算真实的$$p^*(y|x)$$，而这正是判别方法的目标。然而正是因为我们无法准确地获得真实的分布，造成这两种方案在实践中是不同的。先估计p( y)p(x|y)，然后计算p(y|x)(生成方案)，会产生与直接估计p(y|x)不同的结果。也就是说，生成与判别模型的目标都是估计p(y|x)，却是通过不同的路径达到的。

我们关于生成与判别之间差异的深入观点，来自Minka【93】。假如我们拥有一个生成模型$$p_g$$，其参数为$$\theta$$。根据定义，其形式为：

$$
p_g(y, x;\theta)=p_g(y;\theta)p_g(x|y;\theta) (2.12)
$$

但是我们也可以按照概率的链式法则重写$$p_g$$如下：

$$
p_g(y,x;\theta)=p_g(x;\theta)p_g(y|x;\theta),(2.13)
$$

其中，$$p_g(x;\theta)$$和$$p_g(y|x;\theta)$$是通过推断来计算的，即$$p_g(x;\theta)=\sum_yp_g(y,x;\theta)$$以及$$p_g(y|x;\theta)=p_g(y,x;\theta)/p_g(x;\theta)$$。

现在要在同样的联合分布簇上，把这个生成模型与判别模型做比较。为了这么做，我们定义一个关于输入的先验概率$$p(x)$$，使得p(x)可以从$$p_g$$的某个参数配置中产生。就是说，$$p( x)=p_c(x;\theta')=\sum_yp_g(y,x;\theta')$$<font color=red>译注：原文是$$p(x)=p_c(x;\theta')=\sum_ yp_g(y,x|\theta')$$</font>，其中$$\theta'$$往往与(2.13)中的$$\theta$$不同。把这与同样从$$p_g$$中产生的条件分布$$p_c(y|x;\theta)$$组合，即$$p_c(y|x;\theta)=p_g(y,x;\theta)/p_g(x;\theta)$$。那么结果分布是：

$$
p_c(y,x)=p_c(x;\theta')p_c(y|x;\theta) (2.14)
$$

通过比较(2.13)和(2.14)，可以看到条件方案具有更大的灵活性来拟合数据，因为它不要求$$\theta'=\theta$$。直观地，因为(2.13)中的参数$$\theta$$被同时用于输入的分布和条件部分。那么一组参数需要在两方面都表现良好。潜在地，需要损失我们所关心的p(y|x)的准确性，来弥补我们不怎么关心的p(x)的准确性。另一方面，引入了更多的自由度，增加了过拟合的风险，降低了泛化到新数据的能力。

尽管到目前为止我们一直在批判生成模型，它们也有自己的优势。第一，生成模型可以更自然地处理隐藏变量，半标注数据以及未标注数据。在更极端的例子中，当整个数据都未被标注时，生成模型可以按照非监督模式使用。相反，非监督学习在判别模型中不够自然，且扔是一个活跃的研究领域。

第二，在某些例子中生成模型表现得比判别模型好，直观上是因为输入模型p(x)对条件分布的影响是光滑的(smoothing)。 Ng和Jordan【98】争辩道，这一作用在小数据机上尤其显著。对于任何特定的数据集，我们不可能知道谁更有优势。总之，要么问题本身需要一个自然的生成模型，要么需要同时预测输入与输出<font color=red>译注：一般应用假定输入为已知，而只需预测输出</font>，都会使生成模型更被青睐。

因为生成模型的形式为p(y,x)=p(y)p(x|y)，使得通过有向图来表示它更自然。其中在拓扑意义上，输出$$ y$$要在输入之前。相似地，我们将会看到，用无向图来表示判别模型更自然。然而，并非总是如此。无向的生成模型，如马尔科夫随机场(2.32)，以及有向的判别模型，如MEMM(6.2)，有时也会被采用。有时用有向图来表示判别模型也会有用，其中x在y之前。

朴素贝叶斯与逻辑回归之间的关系，正如HMMs和线性链CRFs。正如朴素贝叶斯与逻辑回归是生成-判别对，也存在着HMMs的判别对应物。这一对应物是一种特殊的CRF。我们将在接下来一章中介绍。朴素贝叶斯、逻辑回归、生成模型和CRFs之间的类比，如图2.4所示。

![](/assets/2.4.png)

图2.4 朴素贝叶斯、逻辑回归、HMMS、线性链CRFs、生成模型和广义CRFs之间的关系图