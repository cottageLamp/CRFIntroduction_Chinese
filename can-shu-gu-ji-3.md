# 5.3 并行

随机梯度下降通过只计算少量的样本来加速计算过程。另一种加快计算的方法是并行地计算不同样本的梯度。因为梯度（5.6）是在所有的样本上求和，可以轻易地把计算划分成多个线程，其中每个线程只计算训练样本的一个子集的梯度。如果CRF在多核机器上运行，那么多个线程将并行运行，极大地加快了梯度计算。这一特点为许多常见的机器学习算法所共有，如Chu等【22】指出的。

原理上，人们也可以吧梯度计算分布到多个机器上，而不是同一台机器的多个核。然而在网络中传递大量的参数可能会成为问题。一种潜在的解决发办法是异步地更新参数。这一想法的最近一个例子是把并行计算融入随机梯度方法的【64】。