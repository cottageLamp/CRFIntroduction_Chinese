#2.建模
本章，我们从建模的角度来描述CRFs，阐述了CRF是如何把机构化的输出表示成高维输入向量的分布。可以把CRFs理解成，将逻辑回归分类器扩展到任意的图模型，也可以被理解成生成模型（如隐马尔科夫模型）的判别对应物。（译注：**判别**和**生成**模型是两种在理论上等价（可互相推导得到对方），但建模思路相反的模型）。

我们从对图模型的简单介绍（第2.1节），以及对NLP？？中的生成和判别模型的介绍（第2.2节）开始。然后，我们可以给出了CRF的正式定义，包括常用的线性链（linear chains）（第2.3节），以及通用图结构（第2.4节）。因为CRF的准确性严重依赖于所使用的特征，我们也描述了特征工程常用的一些技巧（第2.5节）（译注：就是把原始输入变成特征，再作为模型的输入）。最后，我们提供两个CRF应用的例子（第2.6节），以及一个宽泛的、关于CRFs应用领域的报告。

##2.1 图模型
图模型是表达和推断多变量概率分布的强大框架。它已经在统计模型的许多领域被证明有用，包括编码理论（coding theory），计算机视觉，知识表达（knowledge representation），贝叶斯统计（Bayesian statistics），以及自然语言处理（广告语也太多了吧）。

直接描述包含许多变量的分布，其代价是昂贵的。假如我们用表（table）来描述n个二值变量的联合分布，需要$$O(2^n)$$个浮点数（建议读者理解一下：每个变量有2种可能的取值，而总共有n个变量，那么总共有$$2^n$$种可能的取值。它这里的意思是：给每种取值赋予一个浮点数，表示其概率）。从图模型的角度看，认为一个分布尽管建立在许多变量之上，但常常可以表示成一些局部方程（local functions）的乘积，而这些方程只依赖于少量的变量（译注：能够相乘，意味着相互独立）。这种分解实际上与变量间的某些条件独立性密切相关——两种信息被轻易地用途来概括。实质上，分解、条件独立与图的结构，这三者构成了图模型框架力量的来源：条件独立性视角主要用于设计模型，而分解视角主要用于设计推断算法。

在本节的余下部分，我们从以上两个视角来介绍图模型，关注那些建立在无向图（undirected graphs)之上的模型。关于更详细、更现代的图模型及其推断算法，可参考Koller 和 Friedman 【57】的教材。

###2.1.1 无向图
我们考虑随机变量集合$$Y$$上的概率分布。我们通过整数$$s\in 1,2,...|Y|$$来对变量进行索引。每个变量$$Y_s\in Y$$的取值范围都是集合$$\mathcal{Y}$$（译注：每个变量可能的取值，只能是$$\mathcal{Y}$$中的成员）。本文我们只考虑离散的$$\mathcal{Y}$$，尽管它也可以是连续的。$$Y$$的一次特定的取值记做$$\pmb{y}_s$$(译注：向量用粗体小写字母表示。$$\pmb{y}_s$$是所有变量的一次取值）。对于$$Y$$中的特定变量$$Y_s$$，$$\pmb{y}_s$$包含了对它的赋值，记做$$y_s$$。记号$$\pmb{1}_{\{y=y'\}}$$表示一个函数，在$$y=y'$$时取1，而在其他时候取0。我们还需要边缘分布的记号。对于某个固定的取值$$y_s$$，我们用求和符号$$\sum_{\pmb{y}\backslash y_s}$$来表示：在$$\pmb{y}$$的全部取值中，那些$$Y_s=y_s$$的取值的概率的和（译注：$$\sum_{\pmb{y}\backslash y_s}$$实际上就是：不考虑其他变量，$$Y_s=y_s$$的概率）。

假定，我们相信一个概率分布$$p$$可以表示成一组因子，记做$$\Psi(\pmb{y}_a)$$的连乘。其中，a是一个整数索引（下标），从1变化到A，而A就是因子的个数。每个因子$$\Psi(\pmb{y}_a)$$只依赖于部分变量$$Y_a\in Y$$。$$\Psi(\pmb{y}_a)$$是一个非负数，可以被看成$$\pmb{y}_a$$的自洽性的度量。自洽性高的取值，其发生的概率就高（译注：$$\pmb{y}_a$$表示一组取值。当这组取值自洽的话，其发生的概率就高，反之则低。例如有两个变量：勤奋和贫穷，1表示“正”，0表示“负”。因为勤奋和贫穷往往是不相恰的，所以取值[1,1]的概率低，而取值[1,0]的概率就高。自洽性的度量实际上就是概率）。这种分解让我们更高效地表示分布$$p$$，因为集合$$Y_a$$要比完整的集合$$Y$$小得多。

一个无向图模型是这样一种概率分布，它根据一组给定的因子来分解模型。正式地，给定$$Y$$的子集$$\{Y_a\}^A_{a=1}$$的集合（译注：是子集的集合，一个子集就是一个因子），一个无向图模型是所有可以写成下式的分布：
$$
p(\pmb y)=\frac{1}{Z}\prod^A_{a=1} \Psi(\pmb{y}_a) （2.1）
$$
其中，对于任意的因子$$\mathcal{F}=\{\Psi(\pmb{y}_a)\}$$，及其对应的所有可能的$$\pmb{y}_a$$，都有$$\Psi(\pmb{y}_a)\geq0$$。（这些因子又被称作**局部函数**或**自洽性函数**。）我们将用**随机场**来表示由某个无向图定义的特定分布。

常数$$Z$$是一个归一化因子，保证分布$$p$$的和为1。它定义如下：
$$
Z=\sum_y \prod^A_{a=1}\Psi(\pmb{y}_a).(2.2)
$$

Z的值，考虑成因子集合$$\mathcal{F}$$的函数的话，也被称作**配分函数（partition function）**。注意，式(2.2)中的求和，需要在爆炸式的$$\pmb y$$的所有可能取值上进行。因此，计算Z通常是不可行的，但是有很多关于估计它的研究（见第4章）。

术语“图模型”的来由，在于式（2.1）所表示的因子分解，可以建紧凑地表示成一张图。**因子图【58】**提供了一个特别自然的构图方法。一个因子图是一个两两连接图$$G=(V,F,E)$$。其中，端点的集合$$V=\{1,2,...,|Y|\}$$索引了模型中的全部随机变量，另一组端点的集合$$F=\{1,2,...,A\}$$索引了所有的因子（译注：图中有两种端点：变量端点和因子端点）。对图的理解是：如果一个变量端点s连接到一个因子端点a，那么在模型中，变量$$Y_s$$就是因子$$\Psi_a$$的一个参数。所以，因子图直接描述了，一个分布是如何被分解成多一个局部函数的乘积的。

我们正式地定义，一个因子图是否“描述”了一个分布。记$$N(a)$$包含了端点a的所有邻接端点，就是那些连接上的变量端点。那么：

---------------
**定义2.1** 仅当存在一组局部方程$$\Psi(\pmb{y}_a)$$，使得$$p$$可以写成：
$$
p(\pmb y)=Z^{-1}\prod_{a\in F}\Psi(\pmb{y}_{N(a)}) （2.3）
$$时，一个分布$$p(\pmb y)$$根据因子图G分解了。

---------------

一组子集（译注：代表了一种分解）描述了无向模型，而一个因子图同样如此。在式（2.1）中，取子集为端点的邻居$$\{Y_{N(a)}|\forall a\in F\}$$。根据式（2.1）定义的无向图模型，对应着所有根据G进行分解所得的分布。（译注：可以这样理解：一个无向图，通过各端点的参数的变化，可以变化出式（2.1）能够表达的全部分布。这里强调了半天，就是想说两个空间是等同的。对于我们应用者来说，也许会受不了这种思维。那么，首先记住那个基本的概率公式（2.1），然后学会利用类似图2.1来形象地理解这个分布。）

 ![](/assets/QQ截图20171228224406.png)
图2.1 带3个变量的因子图


图2.1展示了一个带有3个随机变量的因子图，图中，圆圈是变量端点，而灰色方块是因子端点。我们根据端点的索引进行了标注。这个因子图能够描述所有的带3个变量的分布，前提是对于任意的$$\pmb{y}=(y_1,y_2,y_3)$$,该分布能够写成$$p(y_1,y_2,y_3)=\Psi_1(y_1,y_2)\Psi_2(y_2,y_3)\Psi_3(y_1,y_3)$$的形式。

图模型的因子分解与变量间（在其取值范围里）的条件独立性密切相关。这种联系可通过另一种无向图来理解——马尔科夫网。它直接描述了多元分布的条件独立关系。马尔科夫网只是随机变量的图，不包括因子。现记$$G$$为整数序列$$V=\{1,2,...,|Y|\}$$上的无向图，而$$V$$仍是随机变量的索引。对于某一个索引$$s$$，记$$N(S)$$为它的邻居。那么我们称$$p$$是关于$$G$$的马尔科夫网，仅当它满足局部的马尔科夫特性：对于任意的两个变量$$Y_s,Y_t\in Y$$，$$Y_s$$关于它的邻居，条件独立于$$Y_t$$。（译注：$$Y_s$$只与它的邻居$$Y_{N(S)}$$有关，而独立于任何其他的变量。）。直白地说，这意味着$$Y_{N(S)}$$包含了所有的、用于预测$$Y_s$$的信息。

把所有连接到同一个因子的变量都两两连接起来，可将如式(2.1)的因子分解分布，变成其对应的马尔科夫网。这很显然，因为由式（2.1）而来的条件分布$$p(y_s|\pmb{y}_{N(S)})$$仅仅是那些马尔科夫毯中的变量的函数。



从因子分解的角度看，马尔科夫网存在着不好的模糊性。考虑图2.2（左）的3变量马尔科夫网。

![](/assets/QQ截图20171230123923.png)
<font size=2>图2.2 带有模糊性的马尔科夫网（左）。右边的两种分解都有可能与左图对应。</font>




。
。
。
。
。
。
。










.

