# 2 建模

本章，我们从建模的角度来描述CRFs，阐述了CRF是如何把机构化的输出表示成高维输入向量的分布。可以把CRFs理解成，将逻辑回归分类器扩展到任意的图模型，也可以被理解成生成模型(如隐马尔科夫模型)的判别对应物。<font color=red>译注：**判别**和**生成**模型是两种在理论上等价(可互相推导得到对方)，但建模思路相反的模型</font>。

我们从对图模型的简单介绍(第2.1节)，以及对NLP中的生成和判别模型的介绍(第2.2节)开始。然后，我们可以给出了CRF的正式定义，包括常用的线性链(linear chains)(第2.3节)，以及通用图结构(第2.4节)。因为CRF的准确性严重依赖于所使用的特征，我们也描述了特征工程常用的一些技巧(第2.5节)。最后，我们提供两个CRF应用的例子(第2.6节)，以及一个宽泛的、关于CRFs应用领域的报告。

## 2.1 图模型

图模型是表达和推断多元概率分布的强大框架。它已经在统计模型的许多领域被证明有用，包括编码理论(coding theory)，计算机视觉，知识表达(knowledge representation)，贝叶斯统计(Bayesian statistics)，以及自然语言处理。

直接描述包含许多变量的分布，其代价是昂贵的。假如我们用表(table)来描述n个二值变量的联合分布，需要$$O(2^n)$$个浮点数(建议读者理解一下：每个变量有2种可能的取值，而总共有n个变量，那么总共有$$2^n$$种可能的取值。它这里的意思是：给每种取值赋予一个浮点数，表示其概率)。从图模型的角度看，认为一个分布尽管建立在许多变量之上，但常常可以表示成一些局部方程(local functions)的乘积，而这些方程只依赖于少量的变量。这种分解实际上与变量间的某些条件独立性密切相关——两种信息被轻易地用途来概括。实质上，分解、条件独立与图的结构，这三者构成了图模型框架力量的来源：条件独立性视角主要用于设计模型，而分解视角主要用于设计推断算法。

在本节的余下部分，我们从以上两个视角来介绍图模型，关注那些建立在无向图(undirected graphs)之上的模型。关于更详细、更现代的图模型及其推断算法，可参考Koller 和 Friedman 【57】的教材。

### 2.1.1 无向图

我们考虑随机变量集合$$Y$$上的概率分布。我们通过整数$$s\in 1,2, \cdots |Y|$$来对变量进行索引。每个变量$$Y_s\in Y$$的取值范围都是集合$$\mathcal{Y}$$。本文我们只考虑离散的$$\mathcal{Y}$$，尽管它也可以是连续的。Y的一次特定的取值记做$${y}_s$$。对于Y中的特定变量$$Y_s$$，$$y_s$$包含了对它的赋值，记做$$y_s$$。记号$${1}_{\{y=y'\}}$$表示一个函数，在$$y=y'$$时取1，而在其他时候取0。我们还需要边缘分布的记号。对于某个固定的取值$$y_s$$，我们用求和符号  $$\sum_{y \backslash y_s}$$来表示:：在y的全部取值中，那些$$Y_s=y_s$$的取值的概率的和。

假定，我们相信一个概率分布p可以表示成一组因子，记做$$\Psi(y_a)$$的连乘。其中，a是一个整数索引(下标)，从1变化到A，而A就是因子的个数。每个因子$$\Psi(y_a)$$只依赖于部分变量$$Y_a\in Y$$。$$\Psi(y_a)$$是一个非负数，可以被看成$$y_a$$的自洽性的度量。自洽性高的取值，其发生的概率就高。这种分解让我们更高效地表示分布p，因为集合$$Y_a$$要比完整的集合$$Y$$小得多。

一个无向图模型是这样一种概率分布，它根据一组给定的因子来分解模型。正式地，给定$$Y$$的子集$$\{Y_a\}^A_{a=1}$$的集合，一个无向图模型是所有可以写成下式的分布：

$$
p(y)=\frac{1}{Z}\prod^A_{a=1} \Psi({y}_a) (2.1)
$$

其中，对于任意的因子$$\mathcal{F}=\{\Psi(y_a)\}$$，及其对应的所有可能的$$y_a$$，都有$$\Psi(y_a)\geq0$$。(这些因子又被称作**局部函数**或**自洽性函数**。)我们将用**随机场**来表示由某个无向图定义的特定分布。常数Z是一个归一化因子，保证分布p的和为1。它定义如下：

$$
Z=\sum_y \prod^A_{a=1}\Psi(y_a) (2.2)
$$


Z的值，考虑成因子集合$$\mathcal{F}$$的函数的话，也被称作**配分函数(partition function)**。注意，式(2.2)中的求和，需要在爆炸式的$$ y$$的所有可能取值上进行。因此，计算Z通常是不可行的，但是有很多关于估计它的研究(见第4章)。

术语"图模型”的来由，在于式(2.1)所表示的因子分解，可以建紧凑地表示成一张图。**因子图【58】**提供了一个特别自然的构图方法。一个因子图是一个两两连接图$$G=(V,F,E)$$。其中，节点的集合$$V=\{1,2,...,|Y|\}$$索引了模型中的全部随机变量，另一组节点的集合$$F=\{1,2,...,A\}$$索引了所有的因子。对图的理解是：如果一个变量节点s连接到一个因子节点a，那么在模型中，变量$$Y_s$$就是因子$$\Psi_a$$的一个参数。所以，因子图直接描述了，一个分布是如何被分解成多一个局部函数的乘积的。

我们正式地定义——一个因子图是否"描述”了一个分布？记N(a)包含了所有连接到因子节点a上的变量节点，那么：

------

**定义2.1** 仅当存在一组局部方程$$\Psi(y_a)$$，使得p可以写成：

$$p(y)=Z^{-1}\prod_{a\in F}\Psi(y_{N(a)}) (2.3)$$

时，一个分布p(y)根据因子图G分解了。

------

一组子集描述了无向模型，而一个因子图同样如此。在式(2.1)中，取子集为节点的邻居$$\{Y_N(a)|\forall a  \in F\}$$。根据式(2.1)定义的无向图模型，对应着所有根据G进行分解所得的分布。

![](/assets/2.1.png)

图2.1 带3个变量的因子图

图2.1展示了一个带有3个随机变量的因子图，图中，圆圈是变量节点，而灰色方块是因子节点。我们根据节点的索引进行了标注。这个因子图能够描述所有的带3个变量的分布，前提是对于任意的$$y=(y_1,y_2,y_3)$$，该分布能够写成$$p(y_1,y_2,y_3)=\Psi_1(y_1,y_2)\Psi_2(y_2,y_3)\Psi_3(y_1,y_3)$$的形式。

图模型的因子分解与变量间(在其取值范围里)的条件独立性密切相关。这种联系可通过另一种无向图来理解——马尔科夫网。它直接描述了多元分布的条件独立关系。马尔科夫网只是随机变量的图，不包括因子。现记$$G$$为整数序列$$V=\{1,2,...,|Y|\}$$上的无向图，而$$V$$仍是随机变量的索引。对于某一个索引$$s$$，记$$N(S)$$为它的邻居。那么我们称$$p$$是关于$$G$$的马尔科夫网，仅当它满足局部的马尔科夫特性：对于任意的两个变量$$Y_s,Y_t\in Y$$，$$Y_s$$关于它的邻居独立于$$Y_t$$。

把所有连接到同一个因子的变量都两两连接起来，可将如式(2.1)的分布，变成其对应的马尔科夫网。这很显然，因为由式(2.1)而来的条件分布$$p(y_s|y_{N(S)})$$仅仅是那些马尔科夫毯中的变量的函数。

从因子分解的角度看，马尔科夫网存在着不好的歧义性。考虑图2.2(左)的3变量马尔科夫网。任何按照$$p(y_1,y_2,y_3)\propto f(y_1,y_2,y_3)$$分解的分布，都可能与它对应。然而，我们希望使用更严格的参数化——$$p(y_1,y_2,y_3)=f(y_1,y_2)g(y_2,y_3)h(y_1,y_3)$$。后面这组模型簇是前面的严格子集，且需要更少的数据来获得准确的分布估计<font color=red>译注：参数估计？</font>。然而，马尔科夫网不能区分这两种参数化。相反，因子图无歧义地描述了模型的因子分解。

![](/assets/2.2.png)

图2.2带有歧义的马尔科夫网(左)。右边的两种分解都有可能与左图对应。

### 2.1.2 有向图

无向模型中的局部函数无需带有方向性的概率表达，有向图模型却把分布分解成局部的条件概率分布。记G为有向无环图，$$\pi(s)$$为$$Y\_s$$的所有父节点的序号集合。一个有向图模型是一簇按照如下分解的分布：

$$
p(y)=\prod^S_{s=1}p(y_s|y_{\pi(s)}) (2.4)
$$

我们称$$p(y_s|y_{\pi(s)})$$为**局部条件分布(localconditionaldistributions)**。注意，对于没有父节点的变量，$$\pi(s)$$可以是空的。这时，$$p(y_s|y_{\pi(s)})$$可被理解为$$p(y_s)$$。可以推断$$p$$是合理归一化的。可以这样来理解有向模型——其每个因子都在局部完成了特殊的归一化，使得(1)因子相当于局部变量上的条件分布，且(2)归一化常数Z=1。有向模型常常用于生成模型，我们将在第2.2.3节讲述这一点。有向模型的一个例子是贝叶斯模型(2.7)，被描述在图2.3(左)了。在这些图中，灰节点表示了某些数据集上观测的变量。贯穿本文，我们都将采用这一习惯。

## 2.2 生成与判别模型

本节我们探讨几个已被用于自然语言处理的简单图模型。虽然它们已被熟知，但它们一方面可以澄清前文提到的诸多概念，另一方面也可以说明某些今后讨论CRFs时会遇到的议题。我们尤其关注隐马尔科夫模型(HMM)，因为它与线性链条件随机场密切相关。

本节的主要目的是对比生成与判别模型。将会提到的模型，包括两个生成模型(朴树贝叶斯和HMM)，一个判别模型(逻辑回归模型)。**生成模型**描述了，一个输出向量y以怎样的概率"生成”输入特征x。**判别模型**从相反的方向工作，直接描述了如何利用输入特征x来给输出y赋值。一般来说，这两者可根据贝叶斯法则互相转化。但在实践中却相去甚远，各自隐藏着一些优点(将在2.2.3节讲述)。


### 2.2.1 分类

我们首先讨论**分类**问题——根据给定的一个向量$$x=(x_1,x_2,...,x_K)$$，来预测单一的y变量的离散值(类别标签)。一个简单的方法是，假定当类别标签已知时，所有的特征是独立的。结果是所谓的朴素贝叶斯分类器。它基于如下的联合概率模型：

$$
p(y,x)=p(y)\prod^K_{k=1}p(x_k|y) (2.7)
$$

这个模型可以描述为图2.3(左)的有向模型。为每个特征$$x_k$$定义因子$$\Psi(y)=p(y)$$，以及因子$$\Psi_k(y,x_k)=p(x_k|y)$$,我们也可以写成因子图。这样的因子图如图2.3(右)所示。  

![](/assets/2.3.png)

图2.3 朴素贝叶斯分类器，被当成有向模型(左)，或因子图(右)

逻辑回归(有时在NLP圈子里叫做**最大熵分类器**)是另一个知名的，且很自然地表达为图模型的分类器。该分类器源于将每个类的逻辑概率，log p(y|x)，假设为x的线性函数，以及一个归一化常数。这导致了如下的条件概率：


$$
p(y|x)=\frac{1}{Z(x)}exp\{\theta_y+\sum^K_{j=1}\theta_{y,j}x_j\} (2.8)
$$

其中$$Z(x)=\sum_y exp\{\theta_y+\sum^K_{j=1}\theta_{y,j}x_j\}$$，是归一化常数。而$$\theta_y$$是偏置量，相当于朴素贝叶斯里面的log p(y)。与其像式(2.8)那样为每一个类制定一个权重向量，我们不如采用被所有类共享的一组权重的记号。这一技巧通过定义一组**特征函数(feature functions)**来实现，而这些特征只对某一类时非零。为了达到这个目的，特征权重的特征函数被定义为$$f_{y',j}(y,x)={1}_{\{y'=y\}}x_j$$，而把偏置权重的特征函数定义为$$f_{y'}(y,x)=1_{\{y'=y\}}$$。现在我们可以用$$f_k$$来遍历每个特征函数$$f_{y',j}$$，用$$\theta_k$$来索引对应的权重$$\theta_{y',j}$$。利用这一符号技巧，逻辑回归模型变成了：

$$
p(y|x)=\frac{1}{Z(x)}exp\{\sum^K_{k=1}\theta_k f_k(y,x)\} (2.9)
$$


我们之所以引入这样的记号，是因为它简化了下文介绍CRFs时的记号。<font color=red>译注：(2.8)中的$$\theta_y$$好像丢失了？</font>

### 2.2.2 序列模型

分类器只对单一变量做预测，但图模型的真正用处在于对大量互相关变量的建模能力。本节，我们讨论了可能是最简单的相关性——图模型中的输出变量被排列成一个序列。为了展示该模型的好处，我们讨论一个自然语言处理中的应用——**命名实体识别(named-entity recognition,NER)**。NER是在文本中识别并分类命名实体，包括地点(如China)，人(如George Bush)和组织(如United Nations)。给定一个句子，命名实体识别任务是把其中的单词切分成几段，每一段对应一个实体，然后对该实体进行分类(类别包括人，组织，地点等等)。该问题的挑战性在于，很多实体的字符串很少见，哪怕在一个很大的训练集上。于是，我们只能根据上下文来识别它们。

一种办法是独立地对每个单词进行分类，看它是一个人、地点、组织或者其他(既不是一个实体)。这种办法的缺点在于：给定输入之后，它假定所有的命名实体标签是独立的。实际上，临近单词的标签是相关的。例如，New York是一个地点，Now York Times却是一个组织。一种缓解这种无关性假设的方法，是把输出变量安排到一个线性链中。这是隐马尔科夫模型(HMM)【111】的方法。一个HMM通过假定一个潜在的**状态**序列$$Y=\{y_t\}^T_{t=1}$$ ，来对一序列的观测$$X=\{x_t\}^T_{t=1}$$ 建模。记S为可能状态的有限集，O为可能观测的有限集，即是说，对于任何的t，$$x_t\in O, y_t\in S$$<font color=red>译注：S包含了所有可能的输出值，O包含了所有可能的输入值</font>。在命名实体例子中，t位置的单词就是观测$$x_t$$ ，而$$y_t$$ 是该位置的标签。

为了可行地对联合分布p(y, x)建模，一个HMM做了两个无关性假设。第一，它假设每个状态只依赖于它的前一个状态，即给定$$y_{t-1}$$ 之后， $$y_t$$ 于$$y_1,y_2,...,y_{t-1}$$ 都无关了。第二，它假定每个观测变量$$x_t$$ 只与对应的状态$$y_t$$ 有关。基于这些假设，我们可用三个概率分布来指明一个HMM。第一个，初始状态的概率布p(y1)；第二个，转移概率$$p(y_t|y_{t-1})$$;最后，观测概率$$p(x_t|y_t)$$。总之，状态序列y于观测序列x的联合分布被分解为：

$$
p(y, x)=\prod^T_{t=1}p*y_t|y_{t-1})p(x_t|y_t) (2.10)
$$

为了简化上式的符号，我们创造了"虚拟”初始状态$$y_0$$，它总是0，并是所有状态序列的起点。这让我们把创始状态概率$$p(y_1)$$写成$$p(y_1|y_0)$$。

HMMs已在自然语言处理中用于很多序列标注任务，如part-of-speech tagging, 命名实体识别和信息提取。

### 2.2.3 比较

生成模型和判别模型都描述了(y,x)的分布，却是从不同的方向。生成模型，如朴素贝叶斯分类器和HMM，是一簇按照p(y,x)=p(y)p(x|y)进行分解的联合分布。也就是说，它描述了如何根据标签采样或”生成"特征。判别模型，如逻辑回归模型，是一簇条件分布$$p(y|x)$$。也就是说，直接对分类规则建模。原理上，利用输入的边缘分布p(x) ，一个判别模型可以被转化成联合分布p(y,x)，然而很少需要这么做。

判别和生成模型在概念上的主要区别，就是条件分布p(y|x)没有包含p(x)的模型，而它对分类并没有用。对p(x)建模的困难性在于，它包含了很多高度相关的特征，而这是很难建模的。如在命名实体识别中，朴素的HMM只依赖于单一的特征——单词本身。然而许多单词，特别是专有名词，却从未在训练集中出现过，因而以单词本身作为特征是缺乏足够的信息的。为了对全新单词进行标注，我们想要利用其它的特征，如它的大小写、它的临近单词、它的前后缀、它在预先确定的一组人或地方中的身份(its membership in predetermined lists of people and locations???)，等等。

判别模型的主要优势在于它适合包含丰富的、重叠的特征。为了理解这一点，考虑一簇朴素贝叶斯分布(2.7)。这簇联合分布的条件部分均采用了"逻辑回归的形式"(2.9)。然而还有很多其他的联合模型，有些带有x 之间的复杂的依赖，而条件分布也采用了(2.9)的形式。为了直接对条件分布建模，我们仍然可以认为p(x)是不可知的。判别模型，如CRF，仅对y的条件独立性做假设，以及y如何依赖于x，但是不对x之间的条件独立性做假设。 这一点也可以通过图形的方式来理解。假定我们有关于联合分布p(y,x)的因子图，现在要构建条件分布p(y|x)的因子图，那么，所有只与x有关的因子都可以消失了。它们与条件部分无关，因为它们关于y是常数。

为了在生成模型中包含互相关的特征，我们有两个选择。一是增强模型以表达输入间的相关性，如在每个$$x_t$$之间增加连接。然而很难可操作地这样做。例如，很难想象如何对单词的大小写以及前后缀之间的相关性建模。亦或者，我们也不想去做这个件事，因为我们总是看得到输入的句子。

第二个办法是只做一些简单的相关性假设，如朴素贝叶斯假设。例如，带有朴素贝叶斯假设的HMM采用了$$p(x, y)=\prod^T_{t=1}p(y_t|y_{t-1})\prod^K_{k=1}p(x_{tk}|y_t)$$的形式。这一思路有时很凑效，但也可能很有问题，因为这一独立性假设会影响性能。例如，虽然朴素贝叶斯分类器在文档分类方面表现优秀，它在许多应用中的平均表现要比逻辑回归差【19】。

而且，朴素贝叶斯可以产生差的概率估计。作为说明的例子，想象朴素贝叶斯在一个二分类问题上训练。现在，我们把输入特征向量$$x=(x_1,x_2,...,x_K)$$重复一下，变换成$$x'=(x_1,x_1,x_2,x_2,...,x_K,x_k)$$，然后运行朴素贝叶斯分类器。虽然没有任何新的信息被加入到数据中，这一变换却增加了概率估计的信心。就是说，朴素贝叶斯对p(y|x')的估计，相比于p(y|x)，更倾向远离0.5。

当我们扩展到序列模型的时候，想朴素贝叶斯那样的假设尤其有问题，因为推断过程需要综合模型不同部分的证据。如果序列的每个位置的标签，其概率估计都偏大，那么很难合理地把它们综合起来。

朴素贝叶斯和逻辑回归之间的差别，正是前者是生成的，而后者是判别的。在输入为离散时，这两个分类器在其他方面完全一致。朴素贝叶斯和逻辑回归考虑了相同的假设空间，因为在相同的决策范围里，任何逻辑回归分类器都可以转变成朴素贝叶斯分类器，反之亦然。再者，朴素贝叶斯模型(2.7)与逻辑回归模型(2.9)定义了相同的分布簇。我们可以生成式地表示(2.7)如下：

$$
p(y,x)=\frac{exp\{\sum_k \theta_kf_k(y,x\}}{\sum_{\hat{y},\hat{x}}\theta_kf_k(\hat{y},\hat{x})} (2.11)
$$

这意味着，如果朴素贝叶斯(2.7)按照极大条件似然来训练，我们会获得与逻辑回归一样的分类器。相反，如果按照生成方法来表示逻辑回归，如(2.11)，并按照最大化联合似然p(y,x)来训练，我们会得到与朴素贝叶斯同样的分类器。按照Ng和Jordan【98】的说法，朴素贝叶斯和逻辑回归构成了**生成-判别对(generative-discriminative pair)**。关于最新的生成与判别模型的理论视角，请参考Liang和Jordan【72】。

原理上，我们可能不清楚这两种方案如此不同的原因，毕竟它们之间可通过贝叶斯法则互相转化。如在朴素贝叶斯模型中，是很容易把联合分布p(y)p(x|y)转化成条件分布p(y|x)的。 实际上，该条件分布与逻辑回归模型(2.9)的形式是一样的。另外如果我们想获得关于数据的“真实”生成模型，即真正把数据产生出来的分布$$p^*(y,x)=p^*(y)p^*(x|y)$$，那么我们只需简单地计算真实的$$p^*(y|x)$$，而这正是判别方法的目标。然而正是因为我们无法准确地获得真实的分布，造成这两种方案在实践中是不同的。先估计p( y)p(x|y)，然后计算p(y|x)(生成方案)，会产生与直接估计p(y|x)不同的结果。也就是说，生成与判别模型的目标都是估计p(y|x)，却是通过不同的路径达到的。

我们关于生成与判别之间差异的深入观点，来自Minka【93】。假如我们拥有一个生成模型$$p_g$$，其参数为$$\theta$$。根据定义，其形式为：

$$
p_g(y, x;\theta)=p_g(y;\theta)p_g(x|y;\theta) (2.12)
$$

但是我们也可以按照概率的链式法则重写$$p_g$$如下：

$$
p_g(y,x;\theta)=p_g(x;\theta)p_g(y|x;\theta),(2.13)
$$

其中，$$p_g(x;\theta)$$和$$p_g(y|x;\theta)$$是通过推断来计算的，即$$p_g(x;\theta)=\sum_yp_g(y,x;\theta)$$以及$$p_g(y|x;\theta)=p_g(y,x;\theta)/p_g(x;\theta)$$。

现在要在同样的联合分布簇上，把这个生成模型与判别模型做比较。为了这么做，我们定义一个关于输入的先验概率$$p(x)$$，使得p(x)可以从$$p_g$$的某个参数配置中产生。就是说，$$p( x)=p_c(x;\theta')=\sum_yp_g(y,x;\theta')$$<font color=red>译注：原文是$$p(x)=p_c(x;\theta')=\sum_ yp_g(y,x|\theta')$$</font>，其中$$\theta'$$往往与(2.13)中的$$\theta$$不同。把这与同样从$$p_g$$中产生的条件分布$$p_c(y|x;\theta)$$组合，即$$p_c(y|x;\theta)=p_g(y,x;\theta)/p_g(x;\theta)$$。那么结果分布是：

$$
p_c(y,x)=p_c(x;\theta')p_c(y|x;\theta) (2.14)
$$

通过比较(2.13)和(2.14)，可以看到条件方案具有更大的灵活性来拟合数据，因为它不要求$$\theta'=\theta$$。直观地，因为(2.13)中的参数$$\theta$$被同时用于输入的分布和条件部分。那么一组参数需要在两方面都表现良好。潜在地，需要损失我们所关心的p(y|x)的准确性，来弥补我们不怎么关心的p(x)的准确性。另一方面，引入了更多的自由度，增加了过拟合的风险，降低了泛化到新数据的能力。

尽管到目前为止我们一直在批判生成模型，它们也有自己的优势。第一，生成模型可以更自然地处理隐藏变量，半标注数据以及未标注数据。在更极端的例子中，当整个数据都未被标注时，生成模型可以按照非监督模式使用。相反，非监督学习在判别模型中不够自然，且扔是一个活跃的研究领域。

第二，在某些例子中生成模型表现得比判别模型好，直观上是因为输入模型p(x)对条件分布的影响是光滑的(smoothing)。 Ng和Jordan【98】争辩道，这一作用在小数据机上尤其显著。对于任何特定的数据集，我们不可能知道谁更有优势。总之，要么问题本身需要一个自然的生成模型，要么需要同时预测输入与输出<font color=red>译注：一般应用假定输入为已知，而只需预测输出</font>，都会使生成模型更被青睐。

因为生成模型的形式为p(y,x)=p(y)p(x|y)，使得通过有向图来表示它更自然。其中在拓扑意义上，输出$$ y$$要在输入之前。相似地，我们将会看到，用无向图来表示判别模型更自然。然而，并非总是如此。无向的生成模型，如马尔科夫随机场(2.32)，以及有向的判别模型，如MEMM(6.2)，有时也会被采用。有时用有向图来表示判别模型也会有用，其中x在y之前。

朴素贝叶斯与逻辑回归之间的关系，正如HMMs和线性链CRFs。正如朴素贝叶斯与逻辑回归是生成-判别对，也存在着HMMs的判别对应物。这一对应物是一种特殊的CRF。我们将在接下来一章中介绍。朴素贝叶斯、逻辑回归、生成模型和CRFs之间的类比，如图2.4所示。

![](/assets/2.4.png)

图2.4 朴素贝叶斯、逻辑回归、HMMS、线性链CRFs、生成模型和广义CRFs之间的关系图

## 2.3 线性链CRFs

为了引出线性链CRFs，我们考虑从HMM的联合分布p(y,x)引出的条件分布p(y|x)。关键点在于，这一条件分布是一种具有特殊的特征方程的CRF。

首先，我们来重写HMM的联合分布(2.10)，使其更利于扩展，即：

$$
p(y,x)=\frac{1}{Z}\prod^T_{t=1}exp\left\{\sum_{i,j\in S}\theta_{ij}1_{\{y_t=i\}}1_{\{y_{t-1}=j\}}+\sum_{i\in S}\sum_{o\in O}\mu_{oi}1_{\{y_t=i\}1_{\{x_t=o\}}}\right\} (2.15)
$$

其中，$$\theta=\{\theta_{ij},\mu_{oi}\}$$是分布的实值参数，Z是归一化常数，能使分布的和为1。如果我们不在(2.15)中添加Z，那么参数$$\theta$$有可能带来不合理的关于(y,x)的分布，如当所有参数都是1时。

现在有意思的是，(2.15)(几乎)确切地描述了(2.10)一类的HMMs。每个同类的HMM都可通过如下设置，写成(2.15)的形式：

$$\theta_{ij}=\log p(y'=i|y=j)$$

$$\mu_{oi}=\log p(x=o|y=i)$$

$$Z=1$$

反过来也是正确的，即是说，每个按照(2.15)分解的分布都是HMM。(利用4.1节介绍的前向-反向算法，可构造对应的HMM，从而证明这一点)。因而尽管在参数中增加了灵活性，我们却没有扩大分布簇。

通过使用**特征函数feature functions**，我们可以把(2.15)弄得更紧凑，正如我们在(2.9)的逻辑回归那里一样。每个特征函数都具有形式$$f_k(y_t,y_{t-1},x_t)$$。对于(2.15)，我们需要给每个转移(i,j)一个特征$$f_{ij}(y,y',x)=1_{\{y=i\}}1_{\{y'=j\}}$$，以及给每个“状态-特征对”$$(i,o)$$一个特征$$f_{io}(y,y',x)=1_{\{y=i\}}1_{\{x=o\}}$$。 我们泛泛地用$$f_k$$来引用一个特征，其中$$f_k$$涵盖了全部都的$$f_{ij}$$和全部的$$f_{io}$$。于是，我们可以重写HMM如下：

$$
p(y,x)=\frac{1}{Z}\prod^T_{t=1}\exp\left\{\sum^K_{k=1}\theta_kf_k(y_t,y_{t-1},x_t)\right\} (2.16)
$$

再一次，方程(2.16)定义了与(2.15)完全一样的分布簇，从而也与最初的HMM方程(2.10)一样。

最后一步，是把来自HMM(2.16)的条件分布$$p({y|x})$$写出来，即：

$$
p(y|x)=\frac{p(y,x)}{\sum_{y'}p(y',x)}=\frac{\prod^T_{t=1}exp\left\{\sum^K_{k=1}\theta_kf_k(y_t,y_{t-1},x_t)\right\}}{\sum_{y'}\prod^T_{t=1}exp\left\{\sum^K_{k=1}\theta_kf_k(y'_t,y'_{t-1},x_t)\right\}} (2.17)
$$

(2.17)所描述的条件分布，是线性链CRF的一种特例，即那种只包含当前单词作为特征的。然而，很多线性链CRF使用更为丰富的特征，如前后缀等等。幸运的是，将我们现有的记号扩展并非难事。我们只需简单地允许特征函数包含更多的输入。这导致了我们关于线性链CRFs的一般定义

-----

**定义2.2** 记$$Y,X$$是随机向量，$$\theta=\{\theta_k\}\in \mathcal{R}^K$$是一个参数向量，$$\mathcal{F}=\{f_k(y,y',x_t)\}^K_{k=1}$$为一组实值特征函数。那么**线性链条件随机场**是如下形式的分布p(y|x)：
$$
p(y|x)=\frac{1}{Z}\prod^T_{t=1}exp\left\{\sum^K_{k=1}\theta_kf_k(y_t,y_{t-1},x_t)\right\} (2.18)
$$
其中，$$Z(x)$$是依赖于输入的归一化函数：
$$
Z(x)=\sum_y \prod^T_{t=1}exp\left\{\sum^K_{k=1}\theta_kf_k(y_t,y_{t-1},x_t)\right\} (2.19)
$$

----

<font color=red>译注：线性链条件随机场，好像是一类随机场，实际是一个随机场——结构是定死的。我觉得这是条件随机场最非常核心的问题，本文却并没有阐明。当然，它对输入的引用还是很灵活的。</font>

注意，线性链CRF可以用x和y上的因子图来描述，即

$$
p(y|x)=\frac{1}{Z(x)}\prod^T_{t=1}\Psi_t(y_t,y_{t-1},x_t) (2.20)
$$

其中，局部函数$$\Psi_t$$具有一种特殊的 log-linear形式：

$$
\Psi_t(y_t,y_{t-1},x_t)=exp\left\{\sum^K_{k=1}\theta_kf_k(y_t,y_{t-1},x_t)\right\} (2.21)
$$

当我们在下一节进入一般意义CRF的时候，这会很有用。

一般来说，我们将从数据中学得参数$$\theta$$。这将在第5节讲述。

之前我们已看到，如果一个联合分布p(y,x)像HMM一样分解了，那么对应的条件分布p(y|x)是一个线性链CRF。这一很像HMM的CRF如图2.5所示。

然而，其他类型的线性链CRFs也是有用的。例如，在一个HMM中，状态i到j的转移概率与输入无关，都是$$\log p(y_t=j|y_{t-1}=i)$$。在CRF中，我们可以让转移概率(i,j)依赖于当前的观测向量，这只需添加特征$$1_{\{y_t=j\}}1_{\{y_{t-1}=j\}}1_{\{x_t=o\}}$$。 具有这一转移特征的CRF常常被用于文本处理，如图2.6所示。

实际上，因为CRFs不在乎输入变量$$x_1,\cdots,x_T$$之间的关系，我们可以让因子$$\Psi_t$$依赖于所有的输入x。这不会大破线性图的结构——允许我们把x当成单一的整体变量。结果，特征函数可以写成$$f_k(y_t,y_{t-1},x)$$，从而可以把全部的输入变量x一块考虑。这一事实对CRFs都适用，而不只是对线性链。具有这一结构的线性链如图2.7所示。途中，我们把$$x=(x_1,\cdots,x_T)$$画成一个巨大的观测节点，并被所有的因子依赖，而不是把$$x_1,\cdot,x_T$$画成独立的节点。

![](/assets/2.5.png)

图2.5 来自式(2.17)的类HMM的线性链CRF

![](/assets/2.6.png)

图2.6 转移因子依赖于当前输入的线性链CRF

![](/assets/2.7.png)

图2.7 转移因子依赖于全部输入的线性链CRF

需支出，在我们关于线性链CRF的定义中，特征函数可以从任意时刻依赖于输入，把$$f_k$$关于输入的参数写成了$$x_t$$。$$x_t$$应当被理解成——计算$$t$$时刻特征所需的全部输入<font color=red>译注：而不是t时刻的输入</font>。 例如，如果CRF需要下一时刻的单词$$x_{t+1}$$，那么$$x_t$$应当包含了$$x_{t+1}$$。

最后，归一化常数Z(x)需要在全部 可能的输出序列上求和，包含有爆炸式的大量的项。然而，它可以被前向-反向算法有效地解，正如我们在第4.1节所揭示的。

##2.4 通用CRFs

现在，我们将刚刚探讨的线性链扩展到通用图，以与Lafferty在【63】中对CFR的定义相匹配。概念上，这一扩展是显而易见的。我们只需简单地把线性链因子图变成通用因子图。

--------------

**定义2.3** 记G是在X,Y上的因子图。如果对于X中任意的值x，分布p(y|x)是根据G来分解的，那么$$(X,Y)$$是一个**条件随机场conditional random field**。

--------

那么，每个条件分布p(y|x)都是某些因子图的CRF，包括是平凡的。如果$$F\in\{\Psi_a\}$$是G中的因子的集合，那么一个CRF的条件分布为：

$$
p(y|x)=\frac{1}{Z(X)}\prod^A_{a=1}\Psi_a(y_a,x_a) (2.22)
$$

本定义相比一般无向图的定义(2.1)，差别在于归一化常数Z(x)现在变成了关于输入x的函数。因为条件性趋向于简化图模型，Z(x)有可能被计算，而Z却不是。

正如我们在HMMs和线性链CRFs中的做法，让$$\Psi_a$$是一组特征的线性函数是有用的，即：

$$
\Psi_a(y_a,x_a)=exp\left\{\sum^{K(A)}_{k=1}\theta_{ak}f_{ak}(y_a,x_a)\right\} (2.23)
$$

其中特征函数$$f_{ak}$$和权重$$\theta_{ak}$$都使用了因子的下标a，这是为了强调每个因子都有自己的权重集。一般来说，每个因子也可以拥有自己的特征函数。注意，如果x和y是离散的，那么(2.23)中的log-线性假设并没有带来额外的局限，因为我们可以给(y_a,x_a)的每一个值安排一个指示函数$$f_{ak}$$，类似于我们把HMMs转变成线性链CRF时的做法。

综合(2.22)和(2.23)，可以把log-线性因子CRF的条件分布写成

$$
p(y|x)=\frac{1}{Z(x)}\prod_{\Psi_A\in F}exp\left\{\sum^{K(A)}_{k=1}\theta_{ak}f_{ak}(y_a,x_a)\right\} (2.24)
$$

另外，许多应用模型常常需要参数绑定。以线性链为例，每一时刻的因子$$\Psi_t(y_t,y_{t-1},{x}_t)$$常常使用相同的权重。为了表示这一情况，我们把G的因子划分成$$\mathcal{C}=\{C_1,C_2,\cdots,C_P\}$$，其中每个$$C_P$$是一个**团模板clique template**，是一组共享了特征函数$$\{f_{pk}(x_c,y_c)\}^{K(p)}_{k=1}$$和参数$$\theta_p\in \mathcal{R}^{K(p)}$$的因子。一个使用了团模板的CRF可以写成

$$
p(y|x)=\frac{1}{Z(x)}\prod_{C_p\in\mathcal{C}}\prod_{\Psi_c\in C_p}\Psi_c(x_c,y_c;\theta_p) (2.27)
$$

其中每个模板因子是这样参数化的

$$
\Psi_c(x_c,y_c;\theta_p)=exp\left\{\sum^{K(p)}_{k=1}\theta_{pk}f_{pk}(x_c,y_c\right\} (2.26)
$$

而归一化函数为

$$
Z(x)=\sum_y\prod_{C_p\in\mathcal{C}}\prod_{\Psi_c\in C_p}\Psi_c(x_c,y_c). (2.27)
$$

这一团模板的记号方法即指明了结构重复，也指明了参数绑定。以线性链CRF为例，典型的团模板$$C_0=\{\Psi_t(y_t,y_{t-1},{x}_t)\}^T_{t=1}$$倍整个网络使用，因而$$\mathcal{C}=\{C_0\}$$是元素单一的集合。如果相反地，我们希望给每个因子$$\Psi_t$$分配独立的参数，就像非齐次HMM，那么需要T个模板，即$$\mathcal{C}=\{C_t\}^T_{t=1}, C_t=\{\Psi_t(y_t,y_{t-1},{x}_t)\}$$。

定义通用CRF时，如何给出重复的结构以及参数绑定，是属于最需要考虑的问题。人们推荐了一系列的规范，用于指定团模板，而我们仅仅在这里简单的罗列一下。例如，**动态条件随机场dynamic conditional random field**[140]是一些序列模型，允许在每个时刻拥有多个标签<font color=red>译注：不是指有多个类别，而是有多个变量</font>，而不只是单一的标签，很像动态贝叶斯网络。第二，**关系马尔科夫网relational Markov networks**【142】，是一种用类SQL的语法来指明图结构和参数绑定的通用CRF。**马尔科夫逻辑网Markov logic networks**【113,128】用逻辑式子(logic formulae)来给出无向图的局部函数的分数。实质上，知识库中的每条一阶规则都存在一组参数。MLN的逻辑部分，本质上，可以被看成一种编码惯例，用来指明无向图中的重复结构以及参数绑定。Imperatively define factor graphs【87】使用了完整表达的Turing-complete函数来定义团模板，即给出了模型的结构，也给出了充分统计量$$f_{pk}$$。这些函数灵活地采用了先进的编程思想，包括递归、任意搜索(arbitrary search)、惰性计算以及记忆化。本文采用的团模板的记号，来自于Taskar et al.[142]， Sutton et al. [140]，Richardson 和 Domingos [113]，以及McCallum et al.[87]

##2.5特征工程

<font color=red>不知道怎么翻译这里的专业名词</font>

这一节，我们讲述一些特征工程中的技巧。虽然主要用于语言处理，它们还是很通用的。最主要的权衡很典型——大的特征集可以提高预测的精度，因为决策便捷更加灵活，但却需要更大的内存来保存参数，且可能因为过拟合而降低预测精度。

**标签-观测特征?Label-observation features**.首先，当标签是离散变量，那么团模板$$\mathcal{C}_p$$的特征$$f_{pk}$$常常采用如下的特定形式：

$$
f_{pk}(y_c,x_c)={1}_{\{y_c=\tilde{y}_c\}}q_{pk}(x_c) (2.28)
$$

也就是说，一个特征只在输出正好为$${\tilde{y}}_c$$时才非零，而一旦如此，便只与输入有关。 我们把具有这种形式的特征称为标签-观测特征。本质上可以这么来理解：特征只依赖于输入$${x}_c$$，但每一种输出都有自己的一组权重。这一特征表示法的计算效率也很高，因为计算每个$$q_{pk}$$都可能涉及文本或图片处理，而只需要处理一次，就可用于每一个用到它的特征。为了避免混淆，我们把函数$$q_{pk}({x}_c)$$叫做观测函数，而不是特征。观测函数的例子有“单词$$x_t$$是大写的”或“单词$$x_t$$以ing结尾”。

**Unsupported Features.**使用标签-观测特征可能会带来数量庞大的参数。例如在CRFs的第一个大规模应用中，Sha和Pereira【125】在他们的最佳模型中，使用了3百8十万个参数。其中的很多特城从未在训练数据中出现过——它们总是0。原因在于，许多观测函数只与一小部分的标签相对应。例如在命名实体识别任务中，“单词$$x_t$$是with，而标签$$y_t$$是CITY-NAME”，似乎永远不可能在训练集中为真。我们把它们称为unsupported features。可能很意外，这些特征也可能有用，因为可以给它们赋予负的权重，从而防止给错的标签以高的概率。(降低那些从未出现过的标签序列的分数，将会增加那些出现过的标签序列的概率，所以在后文我们描述的参数估计方法中，会给这些特征以负的权重)。包含unsupported features常常带来精度的少量提升，并以巨大的参数数量为代价。

我们曾利用一个特别的技术，来选择unsupported features的一小部分。这可以看成是使用更少内存来利用的unsupported feature的一次简单探索，可以被称为“unsuported features trick”。它认为许多unsupported features是无用的，因为模型不太可能因为它们的激活而犯错。例如，那个“with”特征不太可能有用，因为with是一个常见的单词，且总是属于OTHER标签(即它不是一个名词)。为了减少参数的数量，我们只保留那些有可能剔除错误的unsupported features。一个简单的方法是：首先训练一个不带unsupported feature的CRF，并在几次迭代后就停下来，使得模型并没有完全训练好。然后考虑那些模型未能给正确答案以高概率的团，给它们增加unsupported features。在上面这个例子中，如果我们发现训练集中有一个样本i，其t位置的序列$$x_t^{(i)}$$是with，而$$y_t^{(i)}$$不是“CITY-NAME”<font color=red>原文是$$y_t^{(i)}$$ is not CITY-NAME。我认为应去掉not。译文则保留了这个not</font>，并且$$p(y_t=CITY-NAME |{x}_T^{(i)})>\epsilon$$时($$\epsilon$$时一个阈值)，我们增加"with"这一特征。

**连线-观测特征和节点-观测特征?Edge-Observation and Node-Observation Features.**为了减少模型中的特征数量，我们可以只在某些团使用标签-观测特征，而不是全部。最常见的两种标签-观测特征是*连线-观测特征*和*节点-观测特征*。考虑一个具有M个观测函数$$\{q_m({x})\}, m\in\{1,2,\cdots,M\}$$的线性链CRF。如果使用了连线-观测特征，那么每个局部函数可以依赖于全部的观测函数。那么，我们可以使用这样的特征：单词$$x_t$$是New，$$y_t$$是LOCATION 且$$y_{t-1}$$也是LOCATION。这会导致模型拥有大量的参数，带来内存消耗和过拟合的缺点。一种解决办法是采用节点-观测特征。使用这一类型的特征，转移因子<font color=red>就是局部函数吧?</font>不在依赖于观测函数。于是我们可以使用类似“$$y_t$$是LOCATION，且$$y_{t-1}$$是LOCATION”，以及“$$x_t$$是NEW，且$$y_t$$是LOCATION”的特征，而不能使用那种一次把$$x_t,y_t,y_{t-1}$$都依赖上的特征。连线-观测特征和节点特征都正式地在表2.1中给出了。一般来说，以上两种特征的选择，需要根据具体的问题来定，如需要考虑观测函数的数量，以及数据集的大小。

![](/assets/table 2.1.png)

**Boundary Labels.**最后一个问题是如何在边缘上取标签，例如一个序列的开始和结尾，或一张画的边缘。有时，边缘上的标签与其他标签不同。例如，大写字母在一个句子的中间意味着是专有名词，但如果是在句子的开始却没有这样的意味。一个简单的办法，是在标签序列的前面加一个特殊的标签——START。这允许模型学习得到关于边缘的特性。例如，如果连线-观测特征也被使用了，那么像“$$y_{t-1}=START$$且$$y_t=PERSON$$且$$x_t$$大写”这样的特征，可以表示，大写这一特征在句子的开始时并不是有效的。

**特征归纳？Feature Induction**上文介绍的“unsupported features trick”是“feature induction”的简化版。McCallum【83】提供了CRFsf 特征归纳的更有条理的方法。其中，模型一开始只有一些基本特征，而训练过程会增加这些特征的连接。另外一个选择是特征选择。一个现代的特征选择方法是$$L_1$$规则化。我们将在第5.1.1介绍它。Lavergne et al.[65]发现，在最好的时候，$$L_1$$可以找到一种模型。它只有1%的参数是非零的，却获得与稠密特征集相当的性能。他们还发现，利用$$L_2$$规则化目标函数，来对$$L_1$$规则化所得的非零特征进行微调，也是有用的。

**Categorical Features类属特征(非数值特征).**如果观测是类属的，而不是有序的，就是说，它们是离散而没有内在的顺序性，那么将它们转化成二值化特征是重要的。例如，很合理将特征$$f_k(y,x_t)$$定义为“如果$$x_t$$是单词dog时，$$f_k=1$$，否则为0”。相反，把$$f_k$$定义为单词$$x_t$$在文本词典中的序号是不合理的。 因而在文本处理中，CRF特征常常是二值化的；而在其他诸如视觉和语音识别中，特征常常是数值的。对于数值特征，标准的做法是通过归一化，使其均值为0而标准差为1，或者把它们二值化，使其变成类属特征。

**Features from Different Time Steps.**我们对于特征$$f_k(y_t,y_{t-1},{x}_t)$$的关注可能遮掩了一点，即通常需要让特征的依赖范围，从最近邻扩展到附近的标签。一个这种特征的例子是“单词$$x_{t+2}$$是Times，而标签$$y_t$$是ORGANIZATION“。这有利于识别名词”New York Times"报纸。同样，也临近特征的组合也是有用的，例如“单词$$x_{t+1}$$和$$x_{t+2}$$是York Times”。

**Features as Backoff回退特征？.**在语言处理中，有时需要在模型中包含冗余因子。例如在线性链CRF中，有人会使用连接因子$$\Psi_t(y_t,y_{t-1},{x}_t)$$的同时，还使用变量因子$$\Psi_t(y_t,{x}_t)$$。虽然只使用连接因子也可以定义同样的分布簇，然而当数据量小于特征的数量时，冗余节点因子却像回退语言模型那样有用。(当拥有百万级的特征时，很多数据是很小的！)当使用冗余特征时，规则化(5.1.1节)是很必要的，因为惩罚大的权重会让权重分布到重叠的特征上。

**Features as Model Combination.**另一种有意思的特征可以是相同任务的更简单方法的结果。例如，如果已经拥有了任务的简单规则库simpl'e rule-base系统(例如这样的规则“1900和2100中间的数字字符串表示一个年份)，那么该系统的输出可被用做CRF的观测函数。另一个例子是名录特征gazetteer features，即其观测函数建立在一个预先建立的列表上，如”如果$$x_t$$出现在了Wikipedia提供的某个城市名单列表中，那么$$q(x_t)=1$$“。

更复杂的例子是把生成模型的输出当做判别模型的输出来用。例如人们可以使用$$f_t(y,{x}_t)=p_{HMM}(y_t=y|x)$$作为特征，其中$$p_{HMM}$$表示某个HMM(在相近数据集训练所得的)所给出的$$y_t=y$$的边缘概率。 让HMM和CRF-with-HMM-feature在同一个数据上训练通常不是一个好的想法，因为HMM需要在它自己的数据集上表现极好，而这会让CRF过分依赖与HMM。这一技术可用于提高某个早前的、同一任务的系统的性能。Bernal et al【7】是这一概念的、在DNA序列中识别基因的一个好例子。

相关的想法是对输入$${x}_t$$进行聚类，用任何方法对语料库中的单词进行聚类，然后用类别标签来作为单词$$x_t$$的附加特征。这种特征在Miller et al.[90]那里取得了好的效果。

**Input-Dependent Structure.**在通用CRF中，有时需要让$$p({y|x})$$的图结构随着输入x变化。关于此的一个简单例子是关于文本处理的“skip-chain CRF”【37,117,133】。其背后的思想是，一旦某个单词在句子中出现了两次，我们希望它们属于相同的标签。于是我们在这两个单词中间增加一条连接特征。这让y之上的图结构依赖于输入x。









