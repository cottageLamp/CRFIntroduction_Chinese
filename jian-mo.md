# 2 建模

本章，我们从建模的角度来描述CRFs，阐述了CRF是如何把机构化的输出表示成高维输入向量的分布。可以把CRFs理解成，将逻辑回归分类器扩展到任意的图模型，也可以被理解成生成模型(如隐马尔科夫模型)的判别对应物。<font color=red>译注：**判别**和**生成**模型是两种在理论上等价(可互相推导得到对方)，但建模思路相反的模型</font>。

我们从对图模型的简单介绍(第2.1节)，以及对NLP中的生成和判别模型的介绍(第2.2节)开始。然后，我们可以给出了CRF的正式定义，包括常用的线性链(linear chains)(第2.3节)，以及通用图结构(第2.4节)。因为CRF的准确性严重依赖于所使用的特征，我们也描述了特征工程常用的一些技巧(第2.5节)。最后，我们提供两个CRF应用的例子(第2.6节)，以及一个宽泛的、关于CRFs应用领域的报告。

## 2.1 图模型

图模型是表达和推断多元概率分布的强大框架。它已经在统计模型的许多领域被证明有用，包括编码理论(coding theory)，计算机视觉，知识表达(knowledge representation)，贝叶斯统计(Bayesian statistics)，以及自然语言处理。

直接描述包含许多变量的分布，其代价是昂贵的。假如我们用表(table)来描述n个二值变量的联合分布，需要$$O(2^n)$$个浮点数(建议读者理解一下：每个变量有2种可能的取值，而总共有n个变量，那么总共有$$2^n$$种可能的取值。它这里的意思是：给每种取值赋予一个浮点数，表示其概率)。从图模型的角度看，认为一个分布尽管建立在许多变量之上，但常常可以表示成一些局部方程(local functions)的乘积，而这些方程只依赖于少量的变量。这种分解实际上与变量间的某些条件独立性密切相关——两种信息被轻易地用途来概括。实质上，分解、条件独立与图的结构，这三者构成了图模型框架力量的来源：条件独立性视角主要用于设计模型，而分解视角主要用于设计推断算法。

在本节的余下部分，我们从以上两个视角来介绍图模型，关注那些建立在无向图(undirected graphs\)之上的模型。关于更详细、更现代的图模型及其推断算法，可参考Koller 和 Friedman 【57】的教材。

### 2.1.1 无向图

我们考虑随机变量集合$$Y$$上的概率分布。我们通过整数$$s\in 1,2, \cdots |Y|$$来对变量进行索引。每个变量$$Y_s\in Y$$的取值范围都是集合$$\mathcal{Y}$$。本文我们只考虑离散的$$\mathcal{Y}$$，尽管它也可以是连续的。$$Y$$的一次特定的取值记做$${y}_s$$。对于$$Y$$中的特定变量$$Y_s$$，$${y}_s$$包含了对它的赋值，记做$$y_s$$。记号$${1}_{\{y=y'\}}$$表示一个函数，在$$y=y'$$时取1，而在其他时候取0。我们还需要边缘分布的记号。对于某个固定的取值$$y_s$$，我们用求和符号  $$\sum_{y \backslash y_s}$$来表示:：在$${y}$$的全部取值中，那些$$Y_s=y_s$$的取值的概率的和。

假定，我们相信一个概率分布$$p$$可以表示成一组因子，记做$$\Psi(y_a)$$的连乘。其中，a是一个整数索引(下标)，从1变化到A，而A就是因子的个数。每个因子$$\Psi(y_a)$$只依赖于部分变量$$Y_a\in Y$$。$$\Psi(y_a)$$是一个非负数，可以被看成$$y_a$$的自洽性的度量。自洽性高的取值，其发生的概率就高。这种分解让我们更高效地表示分布$$p$$，因为集合$$Y_a$$要比完整的集合$$Y$$小得多。

一个无向图模型是这样一种概率分布，它根据一组给定的因子来分解模型。正式地，给定$$Y$$的子集$$\{Y_a\}^A_{a=1}$$的集合，一个无向图模型是所有可以写成下式的分布：

$$
p(y)=\frac{1}{Z}\prod^A_{a=1} \Psi({y}_a) (2.1)
$$

其中，对于任意的因子$$\mathcal{F}=\{\Psi(y_a)\}$$，及其对应的所有可能的$$y_a$$，都有$$\Psi(y_a)\geq0$$。(这些因子又被称作**局部函数**或**自洽性函数**。)我们将用**随机场**来表示由某个无向图定义的特定分布。常数$$Z$$是一个归一化因子，保证分布$$p$$的和为1。它定义如下：

$$
Z=\sum_y \prod^A_{a=1}\Psi(y_a) (2.2)
$$


Z的值，考虑成因子集合$$\mathcal{F}$$的函数的话，也被称作**配分函数(partition function)**。注意，式(2.2)中的求和，需要在爆炸式的$$ y$$的所有可能取值上进行。因此，计算Z通常是不可行的，但是有很多关于估计它的研究(见第4章)。

术语"图模型”的来由，在于式(2.1)所表示的因子分解，可以建紧凑地表示成一张图。**因子图【58】**提供了一个特别自然的构图方法。一个因子图是一个两两连接图$$G=(V,F,E)$$。其中，节点的集合$$V=\{1,2,...,|Y|\}$$索引了模型中的全部随机变量，另一组节点的集合$$F=\{1,2,...,A\}$$索引了所有的因子。对图的理解是：如果一个变量节点s连接到一个因子节点a，那么在模型中，变量$$Y_s$$就是因子$$\Psi_a$$的一个参数。所以，因子图直接描述了，一个分布是如何被分解成多一个局部函数的乘积的。

我们正式地定义——一个因子图是否"描述”了一个分布？记$$N(a)$$包含了所有连接到因子节点a上的变量节点，那么：

------

**定义2.1** 仅当存在一组局部方程$$\Psi(y_a)$$，使得$$p$$可以写成：

$$p(y)=Z^{-1}\prod_{a\in F}\Psi(y_{N(a)}) (2.3)$$

时，一个分布$$p(y)$$根据因子图$$G$$分解了。

------

一组子集描述了无向模型，而一个因子图同样如此。在式(2.1)中，取子集为节点的邻居$$\{Y_N(a)|\forall a  \in F\}$$。根据式(2.1)定义的无向图模型，对应着所有根据$$G$$进行分解所得的分布。

![](/assets/2.1.png)

图2.1 带3个变量的因子图

图2.1展示了一个带有3个随机变量的因子图，图中，圆圈是变量节点，而灰色方块是因子节点。我们根据节点的索引进行了标注。这个因子图能够描述所有的带3个变量的分布，前提是对于任意的$$y=(y_1,y_2,y_3)$$，该分布能够写成$$p(y_1,y_2,y_3)=\Psi_1(y_1,y_2)\Psi_2(y_2,y_3)\Psi_3(y_1,y_3)$$的形式。

图模型的因子分解与变量间(在其取值范围里)的条件独立性密切相关。这种联系可通过另一种无向图来理解——马尔科夫网。它直接描述了多元分布的条件独立关系。马尔科夫网只是随机变量的图，不包括因子。现记$$G$$为整数序列$$V=\{1,2,...,|Y|\}$$上的无向图，而$$V$$仍是随机变量的索引。对于某一个索引$$s$$，记$$N(S)$$为它的邻居。那么我们称$$p$$是关于$$G$$的马尔科夫网，仅当它满足局部的马尔科夫特性：对于任意的两个变量$$Y_s,Y_t\in Y$$，$$Y_s$$关于它的邻居独立于$$Y_t$$。

把所有连接到同一个因子的变量都两两连接起来，可将如式(2.1)的分布，变成其对应的马尔科夫网。这很显然，因为由式(2.1)而来的条件分布$$p(y_s|y_{N(S)})$$仅仅是那些马尔科夫毯中的变量的函数。

从因子分解的角度看，马尔科夫网存在着不好的歧义性。考虑图2.2(左)的3变量马尔科夫网。任何按照$$p(y_1,y_2,y_3)\propto f(y_1,y_2,y_3)$$分解的分布，都可能与它对应。然而，我们希望使用更严格的参数化——$$p(y_1,y_2,y_3)=f(y_1,y_2)g(y_2,y_3)h(y_1,y_3)$$。后面这组模型簇是前面的严格子集，且需要更少的数据来获得准确的分布估计<font color=red>译注：参数估计？</font>。然而，马尔科夫网不能区分这两种参数化。相反，因子图无歧义地描述了模型的因子分解。

![](/assets/2.2.png)

图2.2带有歧义的马尔科夫网(左)。右边的两种分解都有可能与左图对应。

### 2.1.2 有向图

无向模型中的局部函数无需带有方向性的概率表达，有向图模型却把分布分解成局部的条件概率分布。记$$G$$为有向无环图，$$\pi(s)$$为$$Y\_s$$的所有父节点的序号集合。一个有向图模型是一簇按照如下分解的分布：

$$
p(y)=\prod^S_{s=1}p(y_s|y_{\pi(s)}) (2.4)
$$

我们称$$p(y_s|y_{\pi(s)})$$为**局部条件分布(localconditionaldistributions)**。注意，对于没有父节点的变量，$$\pi(s)$$可以是空的。这时，$$p(y_s|y_{\pi(s)})$$可被理解为$$p(y_s)$$。可以推断$$p$$是合理归一化的。可以这样来理解有向模型——其每个因子都在局部完成了特殊的归一化，使得(1)因子相当于局部变量上的条件分布，且(2)归一化常数$$Z=1$$。有向模型常常用于生成模型，我们将在第2.2.3节讲述这一点。有向模型的一个例子是贝叶斯模型(2.7)，被描述在图2.3(左)了。在这些图中，灰节点表示了某些数据集上观测的变量。贯穿本文，我们都将采用这一习惯。

## 2.2 生成与判别模型

本节我们探讨几个已被用于自然语言处理的简单图模型。虽然它们已被熟知，但它们一方面可以澄清前文提到的诸多概念，另一方面也可以说明某些今后讨论CRFs时会遇到的议题。我们尤其关注隐马尔科夫模型(HMM)，因为它与线性链条件随机场密切相关。

本节的主要目的是对比生成与判别模型。将会提到的模型，包括两个生成模型(朴树贝叶斯和HMM)，一个判别模型(逻辑回归模型)。**生成模型**描述了，一个输出向量$$y$$以怎样的概率"生成”输入特征$$x$$。**判别模型**从相反的方向工作，直接描述了如何利用输入特征$$x$$来给输出$$y$$赋值。一般来说，这两者可根据贝叶斯法则互相转化。但在实践中却相去甚远，各自隐藏着一些优点(将在2.2.3节讲述)。


### 2.2.1 分类

我们首先讨论**分类**问题——根据给定的一个向量$$x=(x_1,x_2,...,x_K)$$，来预测单一的$$y$$变量的离散值(类别标签)。一个简单的方法是，假定当类别标签已知时，所有的特征是独立的。结果是所谓的朴素贝叶斯分类器。它基于如下的联合概率模型：

$$
p(y,x)=p(y)\prod^K_{k=1}p(x_k|y) (2.7)
$$

这个模型可以描述为图2.3(左)的有向模型。为每个特征$$x_k$$定义因子$$\Psi(y)=p(y)$$，以及因子$$\Psi_k(y,x_k)=p(x_k|y)$$,我们也可以写成因子图。这样的因子图如图2.3(右)所示。  

![](/assets/2.3.png)

图2.3 朴素贝叶斯分类器，被当成有向模型(左)，或因子图(右)

逻辑回归(有时在NLP圈子里叫做**最大熵分类器**)是另一个知名的，且很自然地表达为图模型的分类器。该分类器源于将每个类的逻辑概率，log$$p(y|x)$$，假设为$$x$$的线性函数，以及一个归一化常数。这导致了如下的条件概率：


$$
p(y|x)=\frac{1}{Z(x)}exp\{\theta_y+\sum^K_{j=1}\theta_{y,j}x_j\} (2.8)
$$

其中$$Z(x)=\sum_y exp\{\theta_y+\sum^K_{j=1}\theta_{y,j}x_j\}$$，是归一化常数。而$$\theta_y$$是偏置量，相当于朴素贝叶斯里面的log$$p(y)$$。与其像式(2.8)那样为每一个类制定一个权重向量，我们不如采用被所有类共享的一组权重的记号。这一技巧通过定义一组**特征函数(feature functions)**来实现，而这些特征只对某一类时非零。为了达到这个目的，特征权重的特征函数被定义为$$f_{y',j}(y,x)={1}_{\{y'=y\}}x_j$$，而把偏置权重的特征函数定义为$$f_{y'}(y,x)=1_{\{y'=y\}}$$。现在我们可以用$$f_k$$来遍历每个特征函数$$f_{y',j}$$，用$$\theta_k$$来索引对应的权重$$\theta_{y',j}$$。利用这一符号技巧，逻辑回归模型变成了：

$$
p(y|x)=\frac{1}{Z(x)}exp\{\sum^K_{k=1}\theta_k f_k(y,x)\} (2.9)
$$


我们之所以引入这样的记号，是因为它简化了下文介绍CRFs时的记号。<font color=red>译注：(2.8)中的$$\theta_y$$好像丢失了？</font>

### 2.2.2 序列模型

分类器只对单一变量做预测，但图模型的真正用处在于对大量互相关变量的建模能力。本节，我们讨论了可能是最简单的相关性——图模型中的输出变量被排列成一个序列。为了展示该模型的好处，我们讨论一个自然语言处理中的应用——**命名实体识别(named-entity recognition,NER)**。NER是在文本中识别并分类命名实体，包括地点(如China)，人(如George Bush)和组织(如United Nations)。给定一个句子，命名实体识别任务是把其中的单词切分成几段，每一段对应一个实体，然后对该实体进行分类(类别包括人，组织，地点等等)。该问题的挑战性在于，很多实体的字符串很少见，哪怕在一个很大的训练集上。于是，我们只能根据上下文来识别它们。

一种办法是独立地对每个单词进行分类，看它是一个人、地点、组织或者其他(既不是一个实体)。这种办法的缺点在于：给定输入之后，它假定所有的命名实体标签是独立的。实际上，临近单词的标签是相关的。例如，New York是一个地点，Now York Times却是一个组织。一种缓解这种无关性假设的方法，是把输出变量安排到一个线性链中。这是隐马尔科夫模型(HMM)【111】的方法。一个HMM通过假定一个潜在的**状态**序列$$Y=\{y_t\}^T_{t=1}$$ ，来对一序列的观测$$X=\{x_t\}^T_{t=1}$$ 建模。记$$S$$为可能状态的有限集，$$O$$为可能观测的有限集，即是说，对于任何的$$t$$，$$x_t\in O, y_t\in S$$<font color=red>译注：$$S$$包含了所有可能的输出值，$$O$$包含了所有可能的输入值</font>。在命名实体例子中，t位置的单词就是观测$$x_t$$ ，而$$y_t$$ 是该位置的标签。

为了可行地对联合分布$$p(y, x)$$ 建模，一个HMM做了两个无关性假设。第一，它假设每个状态只依赖于它的前一个状态，即给定$$y_{t-1}$$ 之后， $$y_t$$ 于$$y_1,y_2,...,y_{t-1}$$ 都无关了。第二，它假定每个观测变量$$x_t$$ 只与对应的状态$$y_t$$ 有关。基于这些假设，我们可用三个概率分布来指明一个HMM。第一个，初始状态的概率布$$p(y1)$$；第二个，转移概率$$p(y_t|y_{t-1})$$;最后，观测概率$$p(x_t|y_t)$$。总之，状态序列$$y$$于观测序列$$x$$的联合分布被分解为：

$$
p(y, x)=\prod^T_{t=1}p*y_t|y_{t-1})p(x_t|y_t) (2.10)
$$

为了简化上式的符号，我们创造了"虚拟”初始状态$$y_0$$，它总是0，并是所有状态序列的起点。这让我们把创始状态概率$$p(y_1)$$写成$$p(y_1|y_0)$$。

HMMs已在自然语言处理中用于很多序列标注任务，如part-of-speech tagging, 命名实体识别和信息提取。

### 2.2.3比较

生成模型和判别模型都描述了$$({y},{x})$$的分布，却是从不同的方向。生成模型，如朴素贝叶斯分类器和HMM，是一簇按照$$p(y,x)=p(y)p(x|y)$$进行分解的联合分布。也就是说，它描述了如何根据标签采样或”生成"特征。判别模型，如逻辑回归模型，是一簇条件分布$$p(y|x)$$。也就是说，直接对分类规则建模。原理上，利用输入的边缘分布$$p(x)$$ ，一个判别模型可以被转化成联合分布$$p(y,x)$$，然而很少需要这么做。

判别和生成模型在概念上的主要区别，就是条件分布$$p(y|x)$$没有包含$$p(x)$$的模型，而它对分类并没有用。对$$p(x)$$建模的困难性在于，它包含了很多高度相关的特征，而这是很难建模的。如在命名实体识别中，朴素的HMM只依赖于单一的特征——单词本身。然而许多单词，特别是专有名词，却从未在训练集中出现过，因而以单词本身作为特征是缺乏足够的信息的。为了对全新单词进行标注，我们想要利用其它的特征，如它的大小写、它的临近单词、它的前后缀、它在预先确定的一组人或地方中的身份(its membership in predetermined lists of people and locations???)，等等。

判别模型的主要优势在于它适合包含丰富的、重叠的特征。为了理解这一点，考虑一簇朴素贝叶斯分布(2.7)。这簇联合分布的条件部分均采用了"逻辑回归的形式"(2.9)。然而还有很多其他的联合模型，有些带有$$x$$ 之间的复杂的依赖，而条件分布也采用了(2.9)的形式。为了直接对条件分布建模，我们仍然可以认为$$p(x)$$是不可知的。判别模型，如CRF，仅对$$y$$的条件独立性做假设，以及$$y$$如何依赖于$$x$$，但是不对$$x$$之间的条件独立性做假设。 这一点也可以通过图形的方式来理解。假定我们有关于联合分布$$p({y,x})$$的因子图，现在要构建条件分布$$p(y|x)$$的因子图，那么，所有只与$$x$$有关的因子都可以消失了。它们与条件部分无关，因为它们关于$$y$$是常数。

为了在生成模型中包含互相关的特征，我们有两个选择。一是增强模型以表达输入间的相关性，如在每个$$x_t$$之间增加连接。然而很难可操作地这样做。例如，很难想象如何对单词的大小写以及前后缀之间的相关性建模。亦或者，我们也不想去做这个件事，因为我们总是看得到输入的句子。

第二个办法是只做一些简单的相关性假设，如朴素贝叶斯假设。例如，带有朴素贝叶斯假设的HMM采用了$$p(x, y)=\prod^T_{t=1}p(y_t|y_{t-1})\prod^K_{k=1}p(x_{tk}|y_t)$$的形式。这一思路有时很凑效，但也可能很有问题，因为这一独立性假设会影响性能。例如，虽然朴素贝叶斯分类器在文档分类方面表现优秀，它在许多应用中的平均表现要比逻辑回归差【19】。



















