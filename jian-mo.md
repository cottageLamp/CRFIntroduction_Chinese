# 2 建模

本章，我们从建模的角度来描述CRFs，阐述了CRF是如何把机构化的输出表示成高维输入向量的分布。可以把CRFs理解成，将逻辑回归分类器扩展到任意的图模型，也可以被理解成生成模型（如隐马尔科夫模型）的判别对应物。（译注：**判别**和**生成**模型是两种在理论上等价（可互相推导得到对方），但建模思路相反的模型）。

我们从对图模型的简单介绍（第2.1节），以及对NLP？？中的生成和判别模型的介绍（第2.2节）开始。然后，我们可以给出了CRF的正式定义，包括常用的线性链（linear chains）（第2.3节），以及通用图结构（第2.4节）。因为CRF的准确性严重依赖于所使用的特征，我们也描述了特征工程常用的一些技巧（第2.5节）（译注：就是把原始输入变成特征，再作为模型的输入）。最后，我们提供两个CRF应用的例子（第2.6节），以及一个宽泛的、关于CRFs应用领域的报告。

## 2.1 图模型

图模型是表达和推断多元概率分布的强大框架。它已经在统计模型的许多领域被证明有用，包括编码理论（coding theory），计算机视觉，知识表达（knowledge representation），贝叶斯统计（Bayesian statistics），以及自然语言处理（广告语也太多了吧）。

直接描述包含许多变量的分布，其代价是昂贵的。假如我们用表（table）来描述n个二值变量的联合分布，需要$$O(2^n)$$个浮点数（建议读者理解一下：每个变量有2种可能的取值，而总共有n个变量，那么总共有$$2^n$$种可能的取值。它这里的意思是：给每种取值赋予一个浮点数，表示其概率）。从图模型的角度看，认为一个分布尽管建立在许多变量之上，但常常可以表示成一些局部方程（local functions）的乘积，而这些方程只依赖于少量的变量（译注：能够相乘，意味着相互独立）。这种分解实际上与变量间的某些条件独立性密切相关——两种信息被轻易地用途来概括。实质上，分解、条件独立与图的结构，这三者构成了图模型框架力量的来源：条件独立性视角主要用于设计模型，而分解视角主要用于设计推断算法。

在本节的余下部分，我们从以上两个视角来介绍图模型，关注那些建立在无向图（undirected graphs\)之上的模型。关于更详细、更现代的图模型及其推断算法，可参考Koller 和 Friedman 【57】的教材。

### 2.1.1 无向图

我们考虑随机变量集合$$Y$$上的概率分布。我们通过整数$$s\in 1,2,...|Y|$$来对变量进行索引。每个变量$$Y_s\in Y$$的取值范围都是集合$$\mathcal{Y}$$（译注：每个变量可能的取值，只能是$$\mathcal{Y}$$中的成员）。本文我们只考虑离散的$$\mathcal{Y}$$，尽管它也可以是连续的。$$Y$$的一次特定的取值记做$$\pmb{y}_s$$\(译注：向量用粗体小写字母表示。$$\pmb{y}_s$$是所有变量的一次取值）。对于$$Y$$中的特定变量$$Y_s$$，$$\pmb{y}_s$$包含了对它的赋值，记做$$y_s$$。记号$$\pmb{1}_{\{y=y'\}}$$表示一个函数，在$$y=y'$$时取1，而在其他时候取0。我们还需要边缘分布的记号。对于某个固定的取值$$y_s$$，我们用求和符号$$\sum_{\pmb{y}\backslash y_s}$$来表示：在$$\pmb{y}$$的全部取值中，那些$$Y_s=y_s$$的取值的概率的和（译注：$$\sum_{\pmb{y}\backslash y_s}$$实际上就是：不考虑其他变量，$$Y_s=y_s$$的概率）。

假定，我们相信一个概率分布$$p$$可以表示成一组因子，记做$$\Psi(\pmb{y}_a)$$的连乘。其中，a是一个整数索引（下标），从1变化到A，而A就是因子的个数。每个因子$$\Psi(\pmb{y}_a)$$只依赖于部分变量$$Y_a\in Y$$。$$\Psi(\pmb{y}_a)$$是一个非负数，可以被看成$$\pmb{y}_a$$的自洽性的度量。自洽性高的取值，其发生的概率就高（译注：$$\pmb{y}_a$$表示一组取值。当这组取值自洽的话，其发生的概率就高，反之则低。例如有两个变量：勤奋和贫穷，1表示“正”，0表示“负”。因为勤奋和贫穷往往是不相恰的，所以取值\[1,1\]的概率低，而取值\[1,0\]的概率就高。自洽性的度量实际上就是概率）。这种分解让我们更高效地表示分布$$p$$，因为集合$$Y_a$$要比完整的集合$$Y$$小得多。

一个无向图模型是这样一种概率分布，它根据一组给定的因子来分解模型。正式地，给定$$Y$$的子集$$\{Y_a\}^A_{a=1}$$的集合（译注：是子集的集合，一个子集就是一个因子），一个无向图模型是所有可以写成下式的分布：


$$
p(\pmb y)=\frac{1}{Z}\prod^A_{a=1} \Psi(\pmb{y}_a) （2.1）
$$


其中，对于任意的因子$$\mathcal{F}=\{\Psi(\pmb{y}_a)\}$$，及其对应的所有可能的$$\pmb{y}_a$$，都有$$\Psi(\pmb{y}_a)\geq0$$。（这些因子又被称作**局部函数**或**自洽性函数**。）我们将用**随机场**来表示由某个无向图定义的特定分布。常数$$Z$$是一个归一化因子，保证分布$$p$$的和为1。它定义如下：


$$
Z=\sum_y \prod^A_{a=1}\Psi(\pmb{y}_a).(2.2)
$$


Z的值，考虑成因子集合$$\mathcal{F}$$的函数的话，也被称作**配分函数（partition function）**。注意，式\(2.2\)中的求和，需要在爆炸式的$$\pmb y$$的所有可能取值上进行。因此，计算Z通常是不可行的，但是有很多关于估计它的研究（见第4章）。

术语“图模型”的来由，在于式（2.1）所表示的因子分解，可以建紧凑地表示成一张图。**因子图【58】**提供了一个特别自然的构图方法。一个因子图是一个两两连接图$$G=(V,F,E)$$。其中，节点的集合$$V=\{1,2,...,|Y|\}$$索引了模型中的全部随机变量，另一组节点的集合$$F=\{1,2,...,A\}$$索引了所有的因子（译注：图中有两种节点：变量节点和因子节点）。对图的理解是：如果一个变量节点s连接到一个因子节点a，那么在模型中，变量$$Y_s$$就是因子$$\Psi_a$$的一个参数。所以，因子图直接描述了，一个分布是如何被分解成多一个局部函数的乘积的。

我们正式地定义——一个因子图是否“描述”了一个分布？记$$N(a)$$包含了所有连接到因子节点a上的变量节点。那么：

\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*

**定义2.1** 仅当存在一组局部方程$$\Psi(\pmb{y}_a)$$，使得$$p$$可以写成：

$$p(\pmb y)=Z^{-1}\prod_{a\in F}\Psi(\pmb{y}_{N(a)}) （2.3）$$

时，一个分布$$p(y)$$根据因子图$$G$$分解了。

\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*

一组子集（译注：代表了一种分解）描述了无向模型，而一个因子图同样如此。在式（2.1）中，取子集为节点的邻居$$\{Y_N(a)|∀a∈F\}$$。根据式（2.1）定义的无向图模型，对应着所有根据$$G$$进行分解所得的分布。（译注：可以这样理解：一个无向图，通过各节点的参数的变化，可以变化出式（2.1）能够表达的全部分布。这里强调了半天，就是想说两个空间是等同的。对于我们应用者来说，也许会受不了这种思维。那么，首先记住那个基本的概率公式（2.1），然后学会利用类似图2.1来形象地理解这个分布。）

![](/assets/QQ截图20171228224406.png)

图2.1 带3个变量的因子图

图2.1展示了一个带有3个随机变量的因子图，图中，圆圈是变量节点，而灰色方块是因子节点。我们根据节点的索引进行了标注。这个因子图能够描述所有的带3个变量的分布，前提是对于任意的$$\pmb{y}=(y_1,y_2,y_3)$$，该分布能够写成$$p(y_1,y_2,y_3)=\Psi_1(y_1,y_2)\Psi_2(y_2,y_3)\Psi_3(y_1,y_3)$$的形式。图模型的因子分解与变量间（在其取值范围里）的条件独立性密切相关。这种联系可通过另一种无向图来理解——马尔科夫网。它直接描述了多元分布的条件独立关系。马尔科夫网只是随机变量的图，不包括因子。现记G为整数序列V={1,2,...,\|Y\|}上的无向图，而V仍是随机变量的索引。对于某一个索引s，记N\(S\)为它的邻居。那么我们称p是关于G的马尔科夫网，仅当它满足局部的马尔科夫特性：对于任意的两个变量Ys,Y\_t\in Y，Y\_s关于它的邻居，条件独立于Y\_t。（译注：Y\_s只与它的邻居Y{N\(S\)}有关，而独立于任何其他的变量。）。直白地说，这意味着Y{N\(S\)}包含了所有的、用于预测Y\_s的信息。把所有连接到同一个因子的变量都两两连接起来，可将如式\\(2.1\\)的因子分解分布，变成其对应的马尔科夫网。这很显然，因为由式（2.1）而来的条件分布p\(y\_s\|\pmb{y}{N\(S\)}\)仅仅是那些马尔科夫毯中的变量的函数。从因子分解的角度看，马尔科夫网存在着不好的歧义性。考虑图2.2（左）的3变量马尔科夫网。任何按照p\(y1,y\_2,y\_3\)\varpropto f\(y\_1,y\_2，y\_3\)分解的分布，都可能与它对应。然而，我们希望使用更严格的参数化——p\(y\_1,y\_2,y\_3\)=f\(y\_1,y\_2\)g\(y\_2,y\_3\)h\(y\_1,y\_3\)。后面这组模型簇是前面的严格子集，且需要更少的数据来获得准确的分布估计（译注：参数估计？）。然而，马尔科夫网不能区分这两种参数化。相反，因子图无歧义地描述了模型的因子分解。!\\[\\]\\(/assets/QQ截图20171230123923.png\\)图2.2带有歧义的马尔科夫网（左）。右边的两种分解都有可能与左图对应。\#\#\#2.1.2有向图无向模型中的局部函数无需带有方向性的概率表达，有向图模型却描述了把分布分解成局部的条件概率分布（译注：不仅分解成许多个相互独立的因子（无向图），还在因子里面存在条件概率（有向图））。记G为有向无环图，\pi\(s\)为Y\_s的所有父节点的序号集合。一个有向图模型是一簇按照如下分解的分布：p\(\pmb y\)=\prod^S{s=1}p\(y\_s\|\pmb{y}{\pi\(s\)}\). \(2.4\)

我们称

p\(p\_s\|\pmb{y}{\pi\(s\)}）为\_\_局部条件分布（localconditionaldistributions）\_\_。注意，对于没有父节点的变量，\pi\(s\)可以是空的。这时，p\(y\_s\|\pmb{y}\_{\pi\(s\)}）可被理解为p\(y\_s）。可以推断p是合理归一化的。可以这样来理解有向模型——其每个因子都在局部完成了特殊的归一化，使得（1）因子相当于局部变量上的条件分布，且（2）归一化常数Z=1。有向模型常常用于生成模型，我们将在第2.2.3节讲述这一点。有向模型的一个例子是贝叶斯模型（2.7），被描述在图2.3（左）了。在这些图中，灰节点表示了某些数据集上观测的变量。贯穿本文，我们都将采用这一习惯（译注：即用灰色节点表示输入）。\#\#\#2.1.3输入与输出本文假设我们预先知道哪些是需要预测的变量。模型中的变量将被划分成输入变量X（测量到的变量），以及输出变量Y（需要被预测的变量）。例如，输入向量\pmb{x}由一个句子的每个单词组成，而\pmb{y}是由每个词的词性标注组成的。我们对在变量集X\cup Y上的分布感兴趣（译注：X和Y允许存在重叠）。我们将扩展之前的符号来兼容这一新情况。例如，一个X和Y上的无向模型可以由下士给出: p\(\pmb x,\pmb y\)=\frac{1}{Z}\prod^A\_{a=1}\Psi\_a\(\pmb x\_a,\pmb y\_a\),\(2.5\)

其中，局部函数

\Psi\_a$$现在变成依赖于Xa⊆X,Ya⊆Y两个子集。归一化常数变成了：

Z=∑xx,yy∏a∈FΨa\(xxa,yya\),\(2.6\)

现在，Z需要在所有xx,yy的取值上求和。

## 2.2生成与判别模型

本节我们探讨几个已被用于自然语言处理的简单图模型。虽然它们已被熟知，但它们一方面可以澄清前文提到的诸多概念，另一方面也可以说明某些今后讨论CRFs时会遇到的议题。我们尤其关注隐马尔科夫模型（HMM），因为它与线性链条件随机场密切相关。

本节的主要目的是对比生成与判别模型。将会提到的模型，包括两个生成模型（朴树贝叶斯和HMM），一个判别模型（逻辑回归模型）。**生成模型**描述了，一个输出向量$$\pmb{y}$$以怎样的概率“生成”输入特征$$\pmb{x}$$。**判别模型**从相反的方向工作，直接描述了如何利用输入特征$$\pmb{x}$$来给输出$$\pmb{y}$$赋值。一般来说，这两者可根据贝叶斯法则互相转化。但在实践中却相去甚远，各自隐藏着一些优点（将在2.2.3节讲述）。

### 2.2.1 分类

我们首先讨论**分类**问题——根据给定的一个向量$$\pmb{x}=(x_1,x_2,...,x_K)$$，来预测单一的$$y$$变量的离散值（类别标签）。一个简单的方法是，假定当类别标签已知时，所有的特征是独立的。结果是所谓的朴素贝叶斯分类器。它基于如下的联合概率模型：


$$
p(y,\pmb{x})=p(y)\prod^K_{k=1}p(x_k|y). (2.7)
$$


这个模型可以描述为图2.3（左）的有向模型。为每个特征$$x_k$$定义因子$$\Psi(y)=p(y)$$，以及因子$$\Psi_k(y,x_k)=p(x_k|y)$$,我们也可以写成因子图。这样的因子图如图2.3（右）所示。  
![](/assets/QQ截图20171230153710.png)

图2.3 朴素贝叶斯分类器，被当成有向模型（左），或因子图（右）

逻辑回归（有时在NLP圈子里叫做**最大熵分类器**）是另一个知名的，且很自然地表达为图模型的分类器。该分类器源于将每个类的逻辑概率，$$log p(y|\pmb{x})$$，假设为$$\pmb{x}$$的线性函数，以及一个归一化常数。这导致了如下的条件概率：


$$
p(y|\pmb{x})=\frac{1}{Z(\pmb{x})}exp\{\theta_y+\sum^K_{j=1}\theta_{y,j}x_j\},(2.8)
$$


其中$$Z(\pmb{x})=\sum_y exp\{\theta_y+\sum^K_{j=1}\theta_{y,j}x_j\}$$，是归一化常数。而$$\theta_y$$是偏置量，相当于朴素贝叶斯里面的log$$p(y)$$。与其像式（2.8）那样为每一个类制定一个权重向量，我们不如采用被所有类共享的一组权重的记号。这一技巧通过定义一组**特征函数\(feature functions\)**来实现，而这些特征只对某一类时非零。为了达到这个目的，特征权重的特征函数被定义为$$f_{y',j}(y,\pmb{x})=\pmb{1}_{\{y'=y\}}x_j$$，而把偏置权重的特征函数定义为$$f_{y'}(y,\pmb{x})=\pmb{1}_{\{y'=y\}}$$。现在我们可以用$$f_k$$来遍历每个特征函数$$f_{y',j}$$，用$$\theta_k$$来索引对应的权重$$\theta_{y',j}$$。利用这一符号技巧，逻辑回归模型变成了：


$$
p(y|\pmb{x})=\frac{1}{Z(\pmb{x})}exp\{\sum^K_{k=1}\theta_k f_k(y,\pmb{x})\}.(2.9)
$$


我们之所以引入这样的记号，是因为它简化了下文介绍CRFs时的记号。（译注：这些记号减少了写作的负担，却增加了阅读的负担。建议读者对比一下（2.8）与（2.9）吧。我也理解不到这么做的好处。另外，（2.8）中的$$\theta_y$$好像丢失了？理解一个模型，至少要对它的参数的量有认识，建议认真记住（2.8）式，同时尽量理解下（2.9））

