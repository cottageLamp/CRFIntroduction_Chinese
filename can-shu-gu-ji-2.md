# 5.2 随机梯度方法

到目前为止，我们讨论过的优化方法适用于整批处理（batch setting）。就是说，它们先扫描完整个训练集后，才能更新模型的参数。如果训练数据的idd样本数量极大，这看起来很浪费。<font color="red">我还有一个经验——会造成梯度极大</font>。我们猜测，训练数据中的大量不同样本会提供近似的关于模型参数的信息，因而有可能不用扫描所有的样本，而是只读取少量样本后，就更新模型的参数。

**随机梯度下降Stochastic gradient descent(SGD)**是一种简单的优化方法，用来利用这一洞见。基本的想法是，在每次迭代里，随机地选择一个训练样本，然后采用这一样本带来的梯度，并使用一个小的步长。在整批处理的方案里，梯度下降通常不是有话的好方法，因为局部的最速下降方向（就是负的梯度）可能指向与最值点完全不同的地方。所以随机梯度方法含有一个有趣的权衡：L-BFGS的单步的迭代方向比SGD的好很多，但SGD方向的计算要快得多。

为了简化符号，我们只给出线性链的SGD。然而，它可以很容易用于任意的图结构——只要训练数据是iid 的。单个训练样本$$(x^{(i)}, y^{(i)})$$的似然的梯度为：

![](/assets/exp-5.20.png)

这与完整的梯度（5.6）一样，只是有两点不同：遍历所有样本的求和被去掉了，以及在规则化项中多出来的因子$$1/N$$。这保证整批的梯度等于单样本梯度的和，即$$\bigtriangledown\ell=\sum^N_{i=1}\bigtriangledown\ell_i$$，其中我们用$$\bigtriangledown\ell_i$$来表示单个样本$$i$$的梯度。

在SGD的每次迭代里，我们随机地选择一个样本$$(x^{(i)}, y^{(i)})$$。然后从原有的向量$$\theta^{(m-1)}$$中计算新的参数向量如下

$$
\theta^{(m)}=\theta^{(m-1)}+\alpha_m\bigtriangledown\ell_i(\theta^{(m-1)}),(5.21)
$$

其中，$$\alpha_m>0$$，是步长参数，用于控制参数们在多大程度上演着梯度方向更新。如果步长过大，参数会在每次迭代里，沿着所选的样本的方向摆动过远。如果$$\alpha_m$$太小，则训练过程会非常慢，甚至在极端的例子中，从数值角度看参数已经收敛了，但其实离最小点还很远呢。

我们希望$$\alpha_m$$随着$$m$$的增大而减小，以让优化算法收敛到某个唯一的答案。随机近似过程【54,115】提供了收敛性的经典结果，即至少要求$$\sum_m\alpha_m=\infty$$以及$$\sum_m\alpha^2_m< \infty$$。即是说，$$\alpha$$应该收敛到0，但是不能过快。采用类似$$\alpha_m\sim \frac{1}{m}$$或$$\alpha_m\sim\frac{1}{\sqrt{m}}$$的步长是最常用的方法，能满足上面的要求。然而，简单地采用$$\alpha_m=1/m$$常常不好，因为第一个步长太大了。实际上，常见的技巧是如下的安排：

$$
\alpha_m=\frac{1}{\sigma^2(m_0+m)},(5.22)
$$

其中，$$m_0$$是一个自由参数。对于这一参数，有一个推荐的方法。Leon Bottou【13】的软件包$$crfsgd$$一次采样一个小的训练数据子集，然后在这一子集上使用各种固定步长$$\alpha$$运行SGD。选择$$\alpha^*$$，使得该子集的数据在一次运算后的似然是最大的，然后通过让$$\alpha_0=\alpha^*$$来确定$$m_0$$。

随机梯度下降算法在神经网络文献中又被称为反向传播。过去的许多年中，发展了大量的调节这一算法的技巧【66】。最近，先进在线优化方法【27,43,126,149】重新引发了关注。它也在线地更新参数，但比简单SGD更复杂。Vishwanathan等【149】是第一个在CRF中使用随机梯度方法的应用。

随机梯度方法的主要缺点是它们需要 tuning，这不同于现成的求解器如共轭梯度和L-BFGS。随机梯度方法也不能用于训练数据不是idd 的relational settings，也不能用于小的数据集。对于合适的数据集，共轭梯度方法可以带来可观的加速。