# 3.算法总览

接下来的两节中，我们将讨论CRFs的推断和参数估计。**参数估计Parameter estimation**是要找到一组参数$$\theta$$，使得分布$$p({y|x},\theta)$$与一组输入输出均已知的训练样本$$D=\{{x}^{(i)},{y}^{(i)}\}^N_{i=1}$$相匹配。我们希望，给定任何一个输入样本$${x}^{(i)}$$，从模型推断出的关于输出的分布$$p({y}|{x}^{(i)},\theta)$$，能“像是”从训练数据中得来的真实的输出$${y}^{(i)}$$。

要量化地来理解这一点，可考虑模型中定义的特征函数。考虑线性链CRF。我们希望，随机地从模型中选择一个输入序列$${x}$$，然后从$p({y|x},\theta)$中采样$${y}$$，触发特征$$f_k(y_t,y_{t-1},{x}_t)$$的概率，能与训练数据中$$f_k$$发生的概率相等。正式地，要求$$f_k$$满足：
$$
\sum^T_{i=1}\sum^T_{t=1}f_k(y_t^{(i)})=\sum^N_{i=1}\sum^T_{t=1}\sum_{y,y'}f_k(y,y',{x}_t^{(i)}p(y_t=y,y_{t-1}=y'|{x}^{(i)}).
$$
重要的是，这一方程组可被看成某个关于参数的目标函数的梯度。这一点是很重要的，因为当我们拥有这一目标函数之后，可以用标准的数值方法来优化它。拥有这一特性的目标函数是如下的似然
$$
\mathcal{l}(\theta)=p({y}^{(i)},|{x}^{(i)},\theta)=\sum^N_{i=1}\sum^T_{t=1}\sum^K_{k=1}\theta_kf_k(y_t^{(i)},y_{t-1}^{(i)},{x}_t^{(i)})-\sum^N_{i=1}\log Z({x}^{(i)}),
$$
这是训练样本在模型意义下的概率，是关于参数的函数。

训练CRFs的标准方法是最大化似然，即寻找$$\hat{\theta}_{ML}=\sup_{\theta}\mathcal{l}(\theta)$$。就是说，$$\hat{\theta}_{ML}$$是最有可能产生训练数据的参数。可将似然对其参数求偏导数并置为0，即得前面讨论的方程组。这恰恰会产生我们刚说的，关于特征的期望。

尽管我们只讨论了线性链CRF的极大似然，同样的想法也适用于通用CRFs。在通用CRFs中，不用链条邻域间的变量的边缘分布$$p(y_t,y_{t-1}|{x},\theta)$$，而是通用图模型中一个因子a上的所有变量$$Y_a$$的边缘分布$$p({y}_a|{x},\theta)$$。

参数估计需要计算上述的边缘分布，在计算上是个大挑战。这是**概率推断probabilistic inference**的任务。一般来说，所谓推断，是要在给定输入$${x}$$和参数$$\theta$$的条件下，计算关于输出$${y}$$的预测。我们需要关注关于推断的两个特别重要的任务：

·计算输出变量的子集$$Y_a$$上的边缘分布$$p({y}|{x},\theta)$$。 $$Y_a$$一般要么包含单一的变量，要么是与某个因子连接的所有变量。关于这一问题，一般是作为计算归一化函数$$Z({x})$$的副产品来计算。

·计算输出$${y}^*=\arg \max_{{y}}p({y}|{x},\theta)$$，即关于输入$${x}$$的最可能的输出。

边缘分布$$p({y}_a|{x},\theta)$$与归一化函数$$Z({x})$$通常用于参数估计。有些参数估计方法，如用limited memory BFGS来优化极大似然时，同时需要边缘分布和归一化函数。也有参数方法，如随机梯度下降法，只需要边缘分布。所谓的Viterbi$${y}^*$$，用于给某个未在训练中出现的输入赋予一组标签。

这一推断任务，可使用标准的图模型方法解决。对于树图模型，可精确地计算这些量，而对于一般的图却只能做近似估计。

接下来的两节将讨论推断和参数估计，包括线性链和通用CRFs。在第4节，我们讨论推断方法，包括对树图的精确方法，以及对一般模型的近似估计方法。从某种角度看，因为CRF是一种无向图模型，使得标准的图模型方法也能适用，但我们关注那些最常用于CRF的近似方法。在第5节，我们讨论参数估计。虽然极大似然理解起来很简单，但计算量却非常大。我们不仅描述以近似推断为基础的极大似然方法，还包括一些其他的近似训练方法，用于增加样本量和模型复杂度的伸缩性。





