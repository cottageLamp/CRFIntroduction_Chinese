#5.1 极大似然

## 5.1.1线性链CRF

线性链CRF的极大似然参数可以用数值优化的方法确定。我们拥有iid训练数据$$\mathcal{D}=\{x^{(i)}, y^{(i)}\}^N_{i=1}$$，其中$$ x^{(i)}=\{x^{(i)}_1,x^{(i)}_2,\cdots,x^{(i)}_T\}$$是一系列输入，而$$ y^{(i)}=\{y^{(i)}_1,y^{(i)}_2,\cdots,y^{(i)}_t\}$$是期望的预测结果。为了简化符号，我们假设每个训练序列$$ x^{(i)}$$的长度都是T。一般来说，每个序列的长度不必相同——也就是说，T依赖于i。下面的讨论可直接扩展以覆盖这种情况。

参数估计一般通过带惩罚项的极大似然来完成。因为我们对条件分布建模了，那么如下的$$\log$$似然，有时也叫**条件$$\log$$似然**，正合适：

$$
\ell(\theta)=\sum^N_{i=1}\log p( y^{(i)}| x^{(i)};\theta) (5.1)
$$

计算极大似然估计，实际是最大化$$\ell(\theta)$$。就是说，所求的估计为$$\hat{\theta}_{ML}=\sup_{\theta}\ell(\theta)$$。

一种理解$$p(y^{(i)}| x^{(i)};\theta)$$的办法，是想象它与某各任意的先验概率$$p( x;\theta)$$结合以构成联合分布概率p(y,x)。然后我们的联合$$\log$$似然为

$$
\log p(y|x;\theta)=\log p({y|x};\theta)+\log p(x;\theta'), (5.2)
$$

注意，项$$p(x;\theta')$$与条件分布的 参数$$\theta$$无关。如果我们不用估计p( x)，那么当计算$$\theta$$的极大似然估计时，那么可以直接去掉(5.2)中的第二项。结果正是(5.1)。

将CRF的模型(2.18)带入(5.1)，我们得到：

$$
\ell(\theta)=\sum^N_{i=1}\sum^T_{t=1}\sum^K_{k=1}\theta_kf_k(y^{(i)}_t,y^{(i)}_{t-1}, x^{(i)}_t)-\sum^N_{i=1}\log Z( x^{(i)}) (5.3)
$$

通常，我们拥有数量庞大的参数，如几十万个。为了避免过拟合，我们使用**规则化regularization**，就是对权重向量过大的模进行惩罚。常见的惩罚项是$$\theta$$的欧几里得范数，以及**规则化参数$$1/2\sigma^2$$来定义惩罚强度。规则化的$$\log$$似然为

$$
\ell=\sum^N_{i=1}\sum^T_{t=1}\sum^K_{k=1}\theta_kf_k(y^{(i)}_t,y^{(i)}_{t-1}, x^{(i)}_t)-\sum^N_{i=1}\log Z( x^{(i)})-\sum^K_{k=1}\frac{\theta^2_k}{2\sigma^2} (5.4)
$$

$$\sigma^2$$是一个自由参数，用来决定对大权重的惩罚力度。其直观想法是避免少数特征就支配了预测结果。根据规则化的写法，规则化可以被理解成最大化了一个后验(MAP)估计，就像$$\theta$$被赋予了均值为0方差为$$\sigma^2I$$的高斯后验分布一样<font color="red">不是特别清楚这里的意思，但也无关紧要。我在实践中用的是等式约束。</font>。确定最佳的规则化参数需要 高计算量的 参数扫描。幸运的是，所得模型的精度并不敏感于$$\sigma^2$$(如，10倍以内不会带来大的影响)。最佳的$$\sigma^2$$与训练数据集的大小有关。对于地5.5节江苏的训练集来说，我们通常去$$\sigma^2=10$$。

也可以用$$L_1$$范数来作为规则化，而这相当于对参数做了double exponential prior假设【44】。这得到如下的带惩罚的似然

$$
\ell'(\theta)=\sum^N_{i=1}\sum^T_{t=1}\sum^K_{k=1}\theta_kf_k(y^{(i)}_t,y^{(i)}_{t-1}, x^{(i)}_t)-\sum^N_{i=1}\log Z( x^{(i)})-\alpha\sum^K_{k=1}|\theta_k| (5.5)
$$

其中，$$\alpha$$是需要调节的规则化参数，就像$$L_2$$范数的$$\sigma^2$$。这种规则化鼓励稀疏的参数，即大多数$$\theta_k$$为0。这对特征选择很有用，以及另外理论上的优势【97】。实践中，用$$L_1$$规则化训练的模型更稀疏，但精度方面与$$L_2$$规则化大致相当【57】。$$L_1$$范数的缺点是它在0处不可导，某种程度上让数值优化变得复杂【3,44,160】。

一般来说，$$\ell(\theta)$$的极大值没有封闭解，因而需要数值优化。(5.4)的偏微分是

$$
\frac{\partial \ell}{\partial \theta_k}=\sum^N_{i=1}\sum^T_{t=1}f_k(y^{(i)}_t,y^{(i)}_{t-1}, x^{(i)}_t)-\sum^N_{i=1}\sum^T_{t=1}\sum_{y,y'}f_k(y,y', x^{(i)}_t)p(y,y'| x^{(i)}_t)-\frac{\theta_k}{\sigma^2} (5.6)
$$

可以把第一项看成$$f_k$$在经验分布$$\tilde{p}$$下的期望。这一分布的定义是

$$
\tilde{p}(y,x)=\frac{1}{N}\sum^N_{i=1} 1_{\{y= y^{(i)}\}} 1_{\{x= x^{(i)}\}} (5.7)
$$

第二项来自于$$\log Z(x)$$的偏导数，相当于$$f_k$$在模型分布$$p(y|x;\theta)\tilde{p}( x)$$下的期望。因此，当规则化极大似然的达到它的解时，梯度为0，意味着这两个期望相等。这一优点是指数家族中极大似然估计的标准结果。

要计算似然$$\ell({\theta)}$$和它的微分，需要我们在第4节介绍的推断技术。首先在似然的计算中，推断被用来计算归一化函数$$Z( x^{(i)})$$，是需要遍历所有可能的标签值的。其次是在微分的计算中，推断被用来计算边缘分布$$p(y,y'| x^{(i)}_t)$$。因为这两个量都依赖于$$ x^{(i)}$$，所以为每个样本计算似然时都要运行一次推断。这是与生成模型(如第2.1.1节介绍的无向生成模型)的一个不同点。极大似然也可以用于无向生成模型的训练，但此时Z只与参数有关，而与输入无关。一次似然的计算就要进行N次推断，而这激发了随机梯度上升方法(5.2节)。

现在我们讨论如何优化$$\ell{(\theta)}$$。函数$$\ell{(\theta)}$$是凹的，这是由于形式如$$g(x)=\log \sum_i \exp x_i$$的函数的凸性。凸性对参数估计很有帮助，因为它意味着每个局部极值点就是全局最优点。另外，如果使用了一个严格凹的规则化，如$$L2$$规则化，那么目标函数也变得严格地凹，这暗示它拥有唯一的全局最优点。

或许沿着梯度(5.6)最速上升是优化$$\ell$$的最简单方法，但这在实践中需要太多次迭代了。牛顿方法的收敛要快得多，因为它利用了似然的曲率，但是需要计算Hessian——二阶微分矩阵。Hessian矩阵的大小是参数个数的平方。既然工程中的应用常常使用几万甚至几百万个参数，简单地保存整个Hessian矩阵是不切实际的。

相反，(5.4)的优化需要对二阶信息做近似。特别成功的方法是准牛顿方法，如BFGS【8】。它从目标函数的一阶微分中计算Hessian矩阵的近似。完整的对Hessian矩阵的$$K\times K$$近似仍然需要二次方的尺寸，所以需要有限内存版本的BFGS——来自Byrd等【17】。共轭梯度是另一个对二阶信息做近似的优化技术，并在CRF中获得了成功。关于有限内存BFGS和共轭梯度的优秀介绍，请参考Nocedal和Wright【100】。也可想成是一个黑箱优化程序that is a drop-in replacement for vanilla gradient ascent。当使用了这些二阶方法之后，基于梯度的优化方法比Lafferty等【63】所介绍的那种原始的iterative scaling方法快得多。这被一些作者【80,92,125,153】实践表明了。最后，置信域方法最近在多项逻辑回归上表现良好【74】，因而或许也对CRF表现良好。

这些优化算法——最速下降，牛顿法，准牛顿法，共轭梯度法和置信域法——是非线性函数的标准数值优化技术。我们把它们看成CRF规则化极大似然的现成方法。这些算法通常需要能够计算目标函数的值和梯度。在我们这里，目标函数值是(5.4)式，而一阶导数在(5.6)式中给出了。这也是我们在第4节，除了边缘概率，还讲述了如何计算归一化函数Z(x)的原因。

最后，我们讨论一下训练线性链模型的计算代价。正如我们将在第4.1节看到的<font color="red">?</font>，单个训练样本的似然和梯度可在$$O(TM^2)$$次计算内，通过前向后向算法获得。其中$$M$$是标签的个数而T是训练样本的长度。因为我们需要为每个训练样本运行一次前向后向算法，所以一次似然和梯度的计算量为$$O(TM^2N)$$。所以总的训练代价为$$O(TM^2NG)$$。其中G是优化过程计算梯度的次数<font color="red">应该是迭代次数</font>。可惜，G依赖于数据集且难以提前估计。对于线性链的batch L-BFGS，这个数一般(但不总)是100。对很多数据集来说，这一计算量是合理的。但如果变量的个数M十分巨大，或者训练样本的个数N十分庞大时，可能会让训练变得昂贵。根据标签的数量，训练CRF可能需要几分钟或是几天——可参考5.5节的例子。

## 5.1.2 通用CRF

通用CRF的参数估计在本质上与线性链一样，只是对模型期望的计算需要更通用的推断算法。首先，我们讨论观测完整的情况，其中训练数据和测试数据相互独立。这时，条件$$\log$$似然，使用2.4节的记法，就是
$$
\ell(\theta)=\sum_{C_p=\mathcal{C}}\sum_{\Psi_c\in C_p}\sum^{K(p)}_{k=1}\theta_{pk}f_{pk}(x_c, y_c)-\log Z(x) (5.8)
$$
本节的方程都不显式地遍历训练样本集，因为如果某个应用刚好拥有 idd 训练样本<font red="red"> idd 是啥？</font>，那么可以用图G中断开的多个子块表示。

$$\log$$似然关于团模板$$C_p$$的参数$$\theta_{pk}$$的偏导数为：

$$
\frac{\partial \ell}{\partial \theta_{pk}}=\sum_{\Psi_c \in C_p}f_{pk}({x_c,y_c})-\sum_{\Psi_c\in C_p}\sum_{ y'_c}f_{pk}({x_c,y'_c})p({y'_c|x}) (5.9)
$$

上面的函数$$\ell(\theta)$$与线性链的具有许多共同的特点。首先，0梯度条件可以解释为“要求充分统计$$F_{pk}({x,y})=\sum_{\Psi_c}f_{pk}({x_c,y_c})$$在经验分布和模型分布下具有相同的期望。其次，函数$$\ell(\theta)$$是凹的，因而可以使用像共轭梯度或L-BFGS这样的二阶最大化技术来解。最后，规则化的方法与线性链如出一辙。

到目前为止的所有讨论，都假设训练数据包含完整的输出变量的值。**潜变量Latent variables**是一些在训练和测试时都未知的变量。在Quatoni等的【109,110】中，拥有潜变量的CRF被称为**隐状态hidden-state**CRF(HCRFs)。关于其他早期的HCRFs，请参考【84,138】。训练带潜变量的CRF要更困难，因为需要计算潜变量的边缘概率来完成推断。

假如我们的CRF的输入为x，y为训练数据集中可观测的变量，而w为另外的潜变量。从而，CRF具有如下形式

$$
p( {y,w|x})=\frac{1}{Z( x)}\prod_{C_p\in \mathcal{C}}\prod_{\Psi_c\in C_p}\Psi_c({x_c,w_c,y_c;\theta_p}) (5.10)
$$

一种在训练时用来最大化的目标函数是如下的边缘似然：

$$
\ell ({\theta})=\log p({y|x})=\log \sum_{ w}p({y,w|x}) (5.11)
$$

第一个问题是如何计算边缘似然$$\ell(\theta)$$，因为如果有许多变量 w，那么求和就不能被直接计算。关键是要意识到，我们只需要遍历训练集中出现过的y，而不是所有可能的y，来计算$$\log \sum_{ w}p({y,w|x})$$。于是，利用原始的CRF(5.10)，并让变量Y都取它们在训练集中的值，得到w的分布：

$$
p(w|y,x)=\frac{1}{Z(y,x)}\prod_{C_p\in\mathcal{C}}\prod_{\Psi_c\in C_p}\Psi_c({x_c,w_c,y_c;\theta_p}) (5.12)
$$

其中，归一化因子为

$$
Z(y,x)=\sum_w\prod_{C_p\in\mathcal{C}}\prod_{\Psi_c\in C_p}\Psi_c({x_c,w_c,y_c;\theta_p}) (5.13)
$$

可以像Z(x)那样去计算这一新的归一化常量Z(y,x)。实际上，Z(y,x)更易于计算，因为它只需遍历w来求和，而Z( x)需要遍历w和y。从图的角度，这相当于说，在图G中固定了y的值后，可以让只剩下w的结构得到简化。

一旦计算了Z(y,x)，边缘似然可以按照如下方式计算

$$
p(y|x)=\frac{1}{Z(x)}\sum_w\prod_{C_p\in \mathcal{C}}\prod_{\Psi_c\in C_p}\Psi_c(x_c,w_c,y_c;\theta_p)=\frac{Z(y,x)}{Z(x)} (5.14)
$$

现在我们已经拥有了计算$$\ell$$的方法，那么来讨论如何关于$$\theta$$来最大化它。因为$$\ell$$一般不再是凸的(log - sum-exp是凸的，但两个log-sum-exp的差却不一定了)，所以求它的最大值是困难的。所以，优化过程一般只能保证得到局部极大点。不管使用了什么优化技术，都要特别小心地初始化模型的初始值，以达到全局最大值。

我们讨论最大化$$\ell$$的两种方法：像Quattoni等【109】那样直接使用梯度；像McCallum等【84】那样使用EM。(另外，这里也很适合使用随机梯度下降，第5.2节)要直接最大化$$\ell$$，我们需要计算梯度。利用下面的事实是最简单的方法。对于任何的函数$$f(\theta)$$，我们有

$$
\frac{df}{d\theta}=f(\theta)\frac{d\log f}{d\theta} (5.15)
$$

这里，我们对$$\log f$$使用了链式法则，并对公式做了调整。将这一点应用到边缘似然$$\ell(\theta)=\log\sum_wp(y,w|x)$$得到

$$
\frac{\partial \ell}{\partial \theta_{pk}}=\frac{1}{\sum_{ w}p({y,w|x})}\sum_{ w}\frac{\partial}{\partial\theta_{pk}}[p({y,w|x})] (5.16)
$$

$$
=\sum_{ w}p({w|y,x})\frac{\partial}{\partial\theta_{pk}}[\log p({y,w|x})].(5.17)
$$

这是完整CRF的梯度的期望，是关于w的。这个表达式可简化为

$$
\frac{\partial\ell}{\partial\theta_{pk}}=\sum_{\Psi_c\in C_p}\sum_{ w'_c}p( w'_c|{y,x})f_k( {y_c,x_c,w'_c})-\sum_{\Psi_c\in C_p}\sum_{ w'_c, y'_c}p({w'_c,y'_c|x_c})f_k({y'_c,x_c,w'_c}) (5.18)
$$

这一梯度要求计算两类边缘概率。第一项包含了边缘概率$$p(w'_c|y,x)$$，正是(5.12)式的收缩CRF的边缘分布。第二项包含了一种不同的边缘概率$$p(w'_c,y'_c|x_c)$$，正是完整CRF所需的边缘概率。只要我们计算了这一梯度，就可以用标准的(像共轭梯度算法)技术来最大化$$\ell$$。对于BFGS，我们的经验是，依赖记忆(memory based)对Hessian矩阵的近似变得有歧义，这是因为凸性被破坏了，就像潜变量CRF这里的情况。有一种实践中的小技巧，当上面的情况发生时，重置对Hessian矩阵的近似。

除此之外，也可以通过期望最大化(EM)来优化$$\ell$$。在EM算法的每次迭代里，当前的 参数向量$$\theta^{(j)}$$通过如下方式更新。首先在E步(E-step)中，按照$$q( w)=p(w|y,x;\theta^{(j)})$$计算得到一个松弛函数。其次在M步(M-step)中，一个新的参数向量$$\theta^{(j+1)}$$计算为

$$
\theta^{(j+1)}=\arg\max_{\theta'}\sum_{ w'}q( w')\log p({y,w'|x};\theta') (5.19)
$$

直接最大化算法与EM算法是显著地相似的。通过吧q带入(5.19)并求导可看到这一点。所得的梯度与直接梯度(5.18)式几乎一样。所不同的是，EM中的分布p(w|y,x)是从预先确定的参数获得的，而不是从最大化过程中提取。我们目前尚缺乏两者在潜变量CRF中的对比的经验。