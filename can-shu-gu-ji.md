# 5.参数估计

这一节，我们讲述如何估计CRF的参数$$\theta=\{\theta_k\}$$。在最典型以、最简单的情况下，数据是完全标注的，但也有研究是关于半监督CRF、带隐藏变量的CRF和关系学习的CRF。

极大似然是一种训练CRF的方法，就是说，要选择参数，使训练数据在模型意义下具有最高的概率。原理上，它与逻辑回归的做法很像。考虑我们在第2节所讲述的这些模型之间的联系，这一点应该不让人意外。主要的区别点在于计算方面：CRF倾向拥有更多参数、更复杂的结构，导致了更高的训练成本。

在树形CRF中，极大似然可基于数值优化过程，以第4.1节江苏的推断算法为子过程。推断算法同时计算了似然和它的梯度。一般来说，似然是关于参数的凸函数，意味着有效的优化过程是现成的，且一定收敛到最优点。

我们从讲述极大似然开始，包含有线性链（第5.1.1节）和通用图结构（第5.1.2节），还包括隐藏变量的情况。我们也将讲述两种加快参数训练的方法：随机梯度下降法（挖掘数据中的 iid 结构，第5.2节）和多线程训练（第5.3节）。

对于通用CRF，精确的极大似然训练是不存在的，因而需要近似过程。泛泛地说，有两种解决问题的策略。一是使用易于计算的函数来近似该似然，叫做**代理似然surrogate likelihood**，再数值地优化该代理函数。第二种方法是边缘概率近似。它在极大似然训练需要精确计算的时候，嵌入一个近似的推断算法来计算边缘分布。这里需要小心，因为近似推断和学习之间存在着微妙而复杂的作用。我们在第5.4节讨论这些。

## 5.1极大似然

### 5.1.1线性链CRF

线性链CRF的极大似然参数可以用数值优化的方法确定。我们拥有iid训练数据$$\mathcal{D}=\{\pmb x^{(i)},\pmb y^{(i)}\}^N_{i=1}$$，其中$$\pmb x^{(i)}=\{x^{(i)}_1,x^{(i)}_2,\cdots,x^{(i)}_T\}$$是一系列输入，而$$\pmb y^{(i)}=\{y^{(i)}_1,y^{(i)}_2,\cdots,y^{(i)}_t\}$$是期望的预测结果。为了简化符号，我们假设每个训练序列$$\pmb x^{(i)}$$的长度都是$$T$$。一般来说，每个序列的长度不必相同——也就是说，$$T$$依赖于$$i$$。下面的讨论可径直扩展以覆盖这种情况。

参数估计一般通过带惩罚项的极大似然来完成。因为我们对条件分布建模了，那么如下的$$\log$$似然，有时也叫**条件$$\log$$似然，正合适：
$$
\mathcal{l}(\theta)=\sum^N_{i=1}\log p(\pmb y^{(i)}|\pmb x^{(i)};\theta). (5.1)
$$
计算极大似然估计，实际是最大化$$\mathcal{l}(\theta)$$。就是说，所求的估计为$$\hat{\theta}_{ML}=\sup_{\theta}\mathcal{l}(\theta)$$。

一种理解$$p(\pmb y^{(i)}|\pmb x^{(i)};\theta)$$的办法，是想象它与某各任意的先验概率$$p(\pmb x;\theta)$$结合以构成联合分布概率$$p(\pmb{y,x})$$。然后我们的联合$$\log$$似然为
$$
\log p(\pmb y|\pmb x;\theta)=\log p(\pmb{y|x};\theta)+\log p(\pmb x;\theta'), (5.2)
$$
注意，项$$p(\pmb x;\theta')$$与条件分布的 参数$$\theta$$无关。如果我们不用估计$$p(\pmb x)$$，那么当计算$$\theta$$的极大似然估计时，那么可以直接去掉（5.2）中的第二项。结果正是（5.1）。

将CRF的模型（2.18）带入（5.1），我们得到：
$$
\mathcal{l}=\sum^N_{i=1}\sum^T_{t=1}\sum^K_{k=1}\theta_kf_k(y^{(i)}_t,y^{(i)}_{t-1},\pmb x^{(i)}_t)-\sum^N_{i=1}\log Z(\pmb x^{(i)}),(5.3)
$$
通常，我们拥有数量庞大的参数，如几十万个。为了避免过拟合，我们使用**规则化regularization**，就是对权重向量过大的模进行惩罚。常见的惩罚项是$$\theta$$的欧几里得范数，以及**规则化参数$$1/2\sigma^2$$来定义惩罚强度。规则化的$$\log$$似然为
$$
\mathcal{l}=\sum^N_{i=1}\sum^T_{t=1}\sum^K_{k=1}\theta_kf_k(y^{(i)}_t,y^{(i)}_{t-1},\pmb x^{(i)}_t)-\sum^N_{i=1}\log Z(\pmb x^{(i)})-\sum^K_{k=1}\frac{\theta^2_k}{2\sigma^2}.(5.4)
$$
$$\sigma^2$$是一个自由参数，用来决定对大权重的惩罚力度。其直观想法是避免少数特征就支配了预测结果。根据规则化的写法，规则化可以被理解成最大化了一个后验(MAP)估计，就像$$\theta$$被赋予了均值为0方差为$$\sigma^2I$$的高斯后验分布一样<font color="red">不是特别清楚这里的意思，但也无关紧要。我在实践中用的是等式约束。</font>。确定最佳的规则化参数需要 高计算量的 参数扫描。幸运的是，所得模型的精度并不敏感于$$\sigma^2$$（如，10倍以内不会带来大的影响）。最佳的$$\sigma^2$$与训练数据集的大小有关。对于地5.5节江苏的训练集来说，我们通常去$$\sigma^2=10$$。

