# 4.3 实现方面的注意点

这一节，我们讲述一些在CRFs推断的实践中尤其重要的技术：稀疏性以及防止数值溢出。

首先，利用模型的稀疏性常常能够加快推断。有两类相关的稀疏性：因子值的稀疏性和特征的稀疏性。首先是关于因子值，记得在线性链时，每次前向更新(4.6)和后向更新(4.9)要被执行$$O(M^2)$$次。就是说，与标签的数量M的二次方有关。相似地在通用CRFs中，如果因子是连接着成对的两个变量，那么一次循环BP的更新也需要$$O(M^2)$$次。然而在某些模型中可更高效地实现推断，因为存在先验知识，知道不是所有的因子的取值$$y_t,y_{t-1}$$都是可能的。就是说，对于许多的取值$$y_t,y_{t-1}$$，因子$$\Psi_t(y_t,y_{t-1}$$总是0。这时，把消息传递迭代变成稀疏矩阵运算可以节省计算量。

另一种有用的稀疏性是特征向量的稀疏性。回忆一下(2.26)，计算一个因子$$\Psi_c( x_c, y_c)$$需要计算 参数向量$$\theta_p$$和特征向量$$ f_c\{f_{pk}(y_c, x_c)|\forall p,\forall k\}$$的内积。一般来说，向量$$ f_c$$的许多元素是0。例如自然语言处理常常包含单词是否出现作为特征。这时，使用稀疏向量方式可以节省大量的计算因子$$\Psi_c$$的时间。类似地，我们可以用稀疏性来减少似然梯度的计算时间，如第5节所讨论的。

还有一个可以加快前向后向算法的技巧，就是将某些参数与某些转移(trainsitions)绑定起来【24】。这减少了模型的转移矩阵的大小，减轻计算量与标签数量的二次方关系。

第二个实现推断时需注意的是如何避免数值溢出。前向后向算法和置信传播的概率值，如$$\alpha_t$$和$$m_{sa}$$，通常比数值的精度还小<font color="red">小于浮点数的数值精度</font>(例如HMM中的$$\alpha_t$$，随着$$t$$以指数的方式趋向于0)。有两个标准的方法来解决这一常见问题。一种方式是将每个$$\alpha_t$$和$$\beta_t$$归一化，从而剔除小的值。这一缩放不会影响对Z(x)的计算，因为可以按照$$Z(x)=p(y'| x)^{-1}\prod_t(\Psi_t(y'_t,y'_{t+1}, x_t))$$来计算，其中$$p(y'| x)^{-1}$$是从(4.31)的边缘概率计算来的。然而实际上，【111】描述了更有效的方法，其中的缩放技巧可用于前向后向算法以及循环BP。不管怎么样，它不影响最后的置信值(values of the beliefs)。

防止数值溢出的第二个方法是在对数域完成计算。即是说，前向递归(4.6)变成：

$$
\log \alpha_t(j)=\bigoplus_{i\in S}\left(\log \Psi_t(j,i,x_t)+\log \alpha_{t-1}(i)\right) (4.38)
$$

其中，$$\bigoplus$$表示$$a\bigoplus b=\log(e^a+e^b)$$。一开始，这似乎不能改进什么，因为数值精度在计算$$e^a$$和$$e^b$$时有所损失。然而，$$\bigoplus$$可以计算成：

$$
a\bigoplus b=a+\log(1+e^{b-a})=b+\log(1+e^{a-b}) (4.39)
$$

当我们选择小一点的指数时，这一运算的数值稳定性要好很多。

初一看，我们喜欢归一化方法胜过对数域方法，因为对数域方法需要$$O(TM^2)$$次调用耗时的$$\log$$和$$\exp$$运算。这对HMMs是对的，但不是CRFs。因为CRFs总归是要调用$$\exp$$来计算$$\Psi_t(y_t,y_{t+1}, x_t)$$，哪怕在归一化方法中。因此在CRFs中，调用这些运算不可避免。在最坏的时候，有$$TM^2$$个这样的$$\Psi_t$$，因而归一化方法需要调用这些特殊的函数$$TM^2$$次，与指数域方法一样。然而，有一些特殊的情况，归一化方法可以更快，如当转移特征不依赖于观测时，那么只有$$M^2$$个不同的$$\Psi_t$$。